{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b7ed041-a82b-4e7d-9e4f-a8efcae2a7ee"
      },
      "source": [
        "# Introduction\n",
        "This notebook contains the 4 (+2 bonus) agents used in the paper \"Using Hippocampal Replay to Consolidate Experiences in Memory-Augmented Reinforcement Learning\"\n",
        "- Random\n",
        "- TD (bonus)\n",
        "- Q-Learning (bonus)\n",
        "- Go-Explore\n",
        "- Go-Explore-Count\n",
        "- Explore-Count\n",
        "\n",
        "It contains the 2 discrete state environments used\n",
        "- Unwalled Maze\n",
        "- Walled Maze"
      ],
      "id": "3b7ed041-a82b-4e7d-9e4f-a8efcae2a7ee"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "000f59b7-0167-4400-a116-1e9fcb2544e0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "from collections import defaultdict"
      ],
      "id": "000f59b7-0167-4400-a116-1e9fcb2544e0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d61ceb7c-eac3-4cfe-a31e-356f0582d624"
      },
      "source": [
        "# Agent Definition"
      ],
      "id": "d61ceb7c-eac3-4cfe-a31e-356f0582d624"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93ca1159-6106-4f25-bf57-1643e64a5781"
      },
      "outputs": [],
      "source": [
        "# this is the memory for the agents which need them\n",
        "memory = defaultdict(lambda: 0)"
      ],
      "id": "93ca1159-6106-4f25-bf57-1643e64a5781"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38ab6955-10f1-4752-acf9-d6436efedf0c"
      },
      "source": [
        "## Agent 1: Random Agent"
      ],
      "id": "38ab6955-10f1-4752-acf9-d6436efedf0c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "545a9b0d-37ce-4375-a410-3936e18da3ce"
      },
      "outputs": [],
      "source": [
        "def RandomAgent(env, **kwargs):\n",
        "    return env.sample()"
      ],
      "id": "545a9b0d-37ce-4375-a410-3936e18da3ce"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6c7bbc6-773e-4cb0-b42f-0eb2cda714a6"
      },
      "source": [
        "## Agent 2: TD Agent"
      ],
      "id": "f6c7bbc6-773e-4cb0-b42f-0eb2cda714a6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b4fc117-756b-41c0-b5e2-89eead91104a"
      },
      "source": [
        "TD-error: $\\delta_t = r_{t} + \\gamma \\max_{a\\in A, a: s_t \\rightarrow s_{t+1}}V(s_{t+1}) - V(s_t)$\n",
        "\n",
        "Value update: $V(s_t) \\leftarrow V(s_t) + \\alpha\\delta_t$"
      ],
      "id": "2b4fc117-756b-41c0-b5e2-89eead91104a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67d91115-7ead-408d-8699-a09167c70a15"
      },
      "outputs": [],
      "source": [
        "def TDAgent(env, **kwargs):\n",
        "    eps = kwargs.get('eps', 1)\n",
        "    GAMMA = 0.99\n",
        "    ALPHA = 1\n",
        "    \n",
        "    statehistory = kwargs.get('statehistory', [])\n",
        "    repeatedstate = kwargs.get('repeatedstate', False)\n",
        "    \n",
        "    curstate = env.staterep()\n",
        "    if repeatedstate:\n",
        "        curstate += str(statehistory.count(env.staterep()))\n",
        "    \n",
        "    if env.reward == 1:\n",
        "        memory[curstate] = 1\n",
        "        return\n",
        "\n",
        "    validmoves = env.getvalidmoves()\n",
        "    bestmove = None\n",
        "    bestvalue = -1\n",
        "    \n",
        "    # choose best move\n",
        "    for move in validmoves:\n",
        "        newenv = copy.deepcopy(env)\n",
        "        newenv.step(move)\n",
        "        nextstate = newenv.staterep()\n",
        "        if repeatedstate:\n",
        "            nextstate += str(statehistory.count(newenv.staterep())+1)\n",
        "            \n",
        "        curvalue = memory[nextstate] \n",
        "        if curvalue > bestvalue:\n",
        "            bestvalue = curvalue\n",
        "            bestmove = move\n",
        "    \n",
        "    # if epsilon, then choose randomly\n",
        "    if eps > np.random.rand():\n",
        "        bestmove = env.sample()\n",
        "        \n",
        "    # update current state value with optimal one-step lookahead estimate\n",
        "    td_error = env.reward + GAMMA*bestvalue - memory[curstate]\n",
        "    memory[curstate] = memory[curstate] + ALPHA*td_error\n",
        "    \n",
        "    return bestmove"
      ],
      "id": "67d91115-7ead-408d-8699-a09167c70a15"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54cf064e-898f-445d-8ed9-4e83cc98ffb7"
      },
      "source": [
        "## Agent 3: Q-Learning Agent"
      ],
      "id": "54cf064e-898f-445d-8ed9-4e83cc98ffb7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d760d1c-eec6-4721-bc42-294588e6d1fc"
      },
      "source": [
        "TD-error: $\\delta_t = r_t + \\gamma\\max_{a\\in A}Q(s_{t+1},a) - Q(s_t, a_t)$\n",
        "\n",
        "Q-learning update: $Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha\\delta_t$"
      ],
      "id": "5d760d1c-eec6-4721-bc42-294588e6d1fc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2357a12a-ad42-43b3-8909-e17f4159be69"
      },
      "outputs": [],
      "source": [
        "def QAgent(env, **kwargs):\n",
        "    eps = kwargs.get('eps', 1)\n",
        "    statehistory = kwargs.get('statehistory', [])\n",
        "    repeatedstate = kwargs.get('repeatedstate', False)\n",
        "    \n",
        "    GAMMA = 0.99\n",
        "    ALPHA = 1\n",
        "    \n",
        "    curstate = env.staterep()\n",
        "    if repeatedstate:\n",
        "        curstate += str(statehistory.count(env.staterep()))\n",
        "    \n",
        "    if env.reward == 1:\n",
        "        for move in env.getvalidmoves():\n",
        "            memory[(curstate, move)] = 1\n",
        "        return\n",
        "\n",
        "    validmoves = env.getvalidmoves()\n",
        "    bestmove = None\n",
        "    bestvalue = -1\n",
        "    \n",
        "    # if epsilon, then choose randomly\n",
        "    if eps > np.random.rand():\n",
        "        bestmove = env.sample()\n",
        "    # else choose best move\n",
        "    else:\n",
        "        for move in validmoves:\n",
        "            curvalue = memory[(curstate, move)] \n",
        "            if curvalue > bestvalue:\n",
        "                bestvalue = curvalue\n",
        "                bestmove = move\n",
        "        \n",
        "    # do a one-step in the next direction\n",
        "    newenv = copy.deepcopy(env)\n",
        "    newenv.step(bestmove)\n",
        "    nextstate = newenv.staterep()\n",
        "    if repeatedstate:\n",
        "        nextstate += str(statehistory.count(newenv.staterep())+1)\n",
        "    td_error = env.reward + GAMMA*np.max([memory[(nextstate, move)] for move in newenv.getvalidmoves()]) - memory[(curstate, bestmove)]\n",
        "        \n",
        "    # update the Q-function\n",
        "    memory[(curstate, bestmove)] = memory[(curstate, bestmove)] + ALPHA*td_error\n",
        "    \n",
        "    return bestmove"
      ],
      "id": "2357a12a-ad42-43b3-8909-e17f4159be69"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "934f2d62-11b0-4150-8a76-14039e20c963"
      },
      "source": [
        "## Agent 4: Go-Explore"
      ],
      "id": "934f2d62-11b0-4150-8a76-14039e20c963"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59265644-3b53-4894-8651-7e34b9b5d1cc"
      },
      "outputs": [],
      "source": [
        "def reward_formula(reward = 0, intrinsic = 0, moves = 0, numselected = 0, numvisits = 0, eps = 1e-20):\n",
        "    # return reward*1000 + intrinsic*10 + np.sqrt(moves) - 100*np.sqrt(numselected+numvisits)\n",
        "    return 1000*reward + 1*np.sqrt(moves) - 100*np.sqrt(numselected+numvisits)"
      ],
      "id": "59265644-3b53-4894-8651-7e34b9b5d1cc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cbb6722-3ca7-43be-870a-8b201122320b"
      },
      "outputs": [],
      "source": [
        "''' Chooses the next best memory state to go to '''\n",
        "def ChooseState(env):\n",
        "    bestvalue = -1e20\n",
        "    bestkey = None\n",
        "    \n",
        "    # choose the best memory based on heuristics\n",
        "    for key in memory:\n",
        "        # do not choose final state as there is nothing left to explore\n",
        "        if memory[key]['reward'] == 1: \n",
        "            continue\n",
        "        \n",
        "        curmem = memory[key]\n",
        "        reward = curmem['reward']\n",
        "        intrinsic = curmem['intrinsic']\n",
        "        moves = curmem['moves']\n",
        "        numselected = curmem.get('numselected', 0)\n",
        "        numvisits = curmem['numvisits']\n",
        "        \n",
        "        value = reward_formula(reward = reward, intrinsic = intrinsic, moves = moves, numselected = numselected, numvisits = numvisits)\n",
        "        if value > bestvalue:\n",
        "            bestvalue = value\n",
        "            bestkey = key\n",
        "    \n",
        "    # generate the trajectory to get the environment state\n",
        "    actionhistory = []\n",
        "    statehistory = []\n",
        "    \n",
        "    for move in memory[bestkey]['actionhistory']:\n",
        "        statehistory.append(env.staterep())\n",
        "        env.step(move)\n",
        "        actionhistory.append(move)\n",
        "        \n",
        "    # increment the selection visit count\n",
        "    if 'numselected' in memory[bestkey]:\n",
        "        memory[bestkey]['numselected'] = memory[bestkey]['numselected'] + 1\n",
        "    \n",
        "    return (actionhistory, statehistory, copy.deepcopy(env))\n",
        "        "
      ],
      "id": "3cbb6722-3ca7-43be-870a-8b201122320b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "897adc24-76f0-4c3f-8e4b-cd8b6cffd26e"
      },
      "outputs": [],
      "source": [
        "''' Chooses the best move based on memory and intrinsic rewards '''\n",
        "def GoExplore(env, **kwargs):\n",
        "    \n",
        "    intrinsic_fn = kwargs.get('intrinsic_fn', None)\n",
        "    replay = kwargs.get('replay', False)\n",
        "    getbestmove = kwargs.get('getbestmove', False)\n",
        "    statehistory = kwargs.get('statehistory', [])\n",
        "    actionhistory = kwargs.get('actionhistory', [])\n",
        "    repeatedstate = kwargs.get('repeatedstate', False)\n",
        "    \n",
        "    # if no intrinsic guiding value, then do without intrinsic motivation\n",
        "    if intrinsic_fn is not None:\n",
        "        intrinsic_value = intrinsic_fn(env)\n",
        "    else:\n",
        "        intrinsic_value = 0\n",
        "        \n",
        "    curmoves = env.numsteps\n",
        "    curreward = env.reward\n",
        "\n",
        "    curstate = env.staterep()\n",
        "    if repeatedstate:\n",
        "        curstate += str(statehistory.count(env.staterep()))\n",
        "        \n",
        "    # if this state is not present in memory (should only happen for start state), add it in\n",
        "    if curstate not in memory:\n",
        "        memory[curstate] = {'statehistory': statehistory+[], 'reward': curreward, 'intrinsic': intrinsic_value, 'moves': curmoves, 'numvisits': 0, 'numselected': 0, 'actionhistory': actionhistory+[]}\n",
        "        \n",
        "    curmemory = memory[curstate]\n",
        "    \n",
        "    # only increment memory if not doing replay\n",
        "    if replay:\n",
        "        curmemory['numvisits'] = 0\n",
        "        curmemory['numselected'] = 0\n",
        "    else:\n",
        "        curmemory['numvisits'] = curmemory['numvisits'] + 1\n",
        "    \n",
        "    # if completed, no need to continue to next move selection\n",
        "    if env.done:\n",
        "        # if there is positive reward, then make intrinsic become the extrinsic reward\n",
        "        if env.reward > 0:\n",
        "            curmemory['intrinsic'] = env.reward\n",
        "        return\n",
        "\n",
        "    # if not completed, continue to select next move\n",
        "    validmoves = env.getvalidmoves()\n",
        "    \n",
        "    # if no valid moves, no need to continue to next move selection\n",
        "    if validmoves == []:\n",
        "        return\n",
        "    \n",
        "    bestmove = None\n",
        "    bestvalue = -1e20\n",
        "    bestintrinsic = -1e20\n",
        "    \n",
        "    # choose best move\n",
        "    for move in validmoves:\n",
        "        newenv = copy.deepcopy(env)\n",
        "        newenv.step(move)\n",
        "        \n",
        "        nextmoves = newenv.numsteps\n",
        "        nextreward = newenv.reward\n",
        "        nextmemory = None\n",
        "        \n",
        "        nextstate = newenv.staterep()\n",
        "        if repeatedstate:\n",
        "            nextstate += str(statehistory.count(newenv.staterep())+1)\n",
        "        \n",
        "        if nextstate in memory:\n",
        "            nextmemory = memory[nextstate] \n",
        "            # update the nextmemory if agent has a better reward\n",
        "            if nextreward > nextmemory['reward']:\n",
        "                nextmemory['reward'] = nextreward\n",
        "                nextmemory['moves'] = curmoves + 1\n",
        "                nextmemory['numvisits'] = 0\n",
        "                nextmemory['numselected'] = 0\n",
        "                nextmemory['actionhistory'] = actionhistory+[move]\n",
        "                nextmemory['statehistory'] = statehistory+[newenv.staterep()]\n",
        "                \n",
        "            # update the nextmemory if agent has similar reward but fewer number of moves\n",
        "            elif nextreward == nextmemory['reward'] and nextmemory['moves'] > curmoves + 1:\n",
        "                nextmemory['moves'] = curmoves + 1\n",
        "                nextmemory['numvisits'] = 0\n",
        "                nextmemory['numselected'] = 0\n",
        "                nextmemory['actionhistory'] = actionhistory+[move]\n",
        "                nextmemory['statehistory'] = statehistory+[newenv.staterep()]\n",
        "                \n",
        "        # start a new memory if this is a new state\n",
        "        else:\n",
        "            # if no intrinsic guiding value, then do without intrinsic motivation\n",
        "            if intrinsic_fn is not None:\n",
        "                next_intrinsic_value = intrinsic_fn(newenv)\n",
        "            else:\n",
        "                next_intrinsic_value = 0\n",
        "            memory[nextstate] = {'statehistory': statehistory+[newenv.staterep()], 'reward': nextreward, 'intrinsic': next_intrinsic_value, 'moves': curmoves + 1, 'numvisits': 0, 'numselected': 0, 'actionhistory': actionhistory+[move]}\n",
        "            nextmemory = memory[nextstate]\n",
        "                \n",
        "        # best intrinsic is the highest intrinsic value of all 1-step connections\n",
        "        bestintrinsic = max(bestintrinsic, nextmemory['intrinsic'])\n",
        "        \n",
        "        # determine the next square to visit via a heuristic\n",
        "        reward = nextmemory['reward'] \n",
        "        moves = nextmemory['moves'] \n",
        "        numvisits = nextmemory['numvisits']\n",
        "        intrinsic = nextmemory['intrinsic']\n",
        "        \n",
        "        totalvalue = reward_formula(reward = reward, intrinsic = intrinsic, moves = moves, numselected = 0, numvisits = numvisits)\n",
        "      \n",
        "        if totalvalue > bestvalue or bestvalue is None:\n",
        "            bestvalue = totalvalue\n",
        "            bestmove = move\n",
        "            \n",
        "    # update the one-step lookahead for intrinsic value\n",
        "    curmemory['intrinsic'] = bestintrinsic*0.99\n",
        "    \n",
        "    if getbestmove:\n",
        "        return bestmove\n",
        "    else:\n",
        "        return np.random.choice(validmoves)"
      ],
      "id": "897adc24-76f0-4c3f-8e4b-cd8b6cffd26e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9771605f-ad3f-413b-8680-196b06bb2fae"
      },
      "outputs": [],
      "source": [
        "''' Chooses Best Move '''\n",
        "def GoExploreCount(env, **kwargs):\n",
        "    return GoExplore(env, getbestmove = True, **kwargs)"
      ],
      "id": "9771605f-ad3f-413b-8680-196b06bb2fae"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d04090b3-345e-471b-ad65-264d1f070865"
      },
      "source": [
        "## Agent 5: Count Agent"
      ],
      "id": "d04090b3-345e-471b-ad65-264d1f070865"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88b2d20c-e678-40ad-8ede-930afe969646"
      },
      "outputs": [],
      "source": [
        "''' Chooses the best move based on memory and intrinsic rewards '''\n",
        "def CountAgent(env, **kwargs):\n",
        "    \n",
        "    intrinsic_fn = kwargs.get('intrinsic_fn', None)\n",
        "    replay = kwargs.get('replay', False)\n",
        "    statehistory = kwargs.get('statehistory', [])\n",
        "    actionhistory = kwargs.get('actionhistory', [])\n",
        "    repeatedstate = kwargs.get('repeatedstate', False)\n",
        "    \n",
        "    # if no intrinsic guiding value, then do without intrinsic motivation\n",
        "    if intrinsic_fn is not None:\n",
        "        intrinsic_value = intrinsic_fn(env)\n",
        "    else:\n",
        "        intrinsic_value = 0\n",
        "        \n",
        "    curmoves = env.numsteps\n",
        "    curreward = env.reward\n",
        "\n",
        "    curstate = env.staterep()\n",
        "    if repeatedstate:\n",
        "        curstate += str(statehistory.count(env.staterep()))\n",
        "\n",
        "    # if this state is not present in memory (should only happen for start state), add it in\n",
        "    if curstate not in memory:\n",
        "        memory[curstate] = {'statehistory': statehistory+[], 'reward': curreward, 'intrinsic': intrinsic_value, 'moves': curmoves, 'numvisits': 0, 'actionhistory': actionhistory+[]}\n",
        "        \n",
        "    curmemory = memory[curstate]\n",
        "    \n",
        "    # only increment memory if not doing replay\n",
        "    if replay:\n",
        "        curmemory['numvisits'] = 0\n",
        "    else:\n",
        "        curmemory['numvisits'] = curmemory['numvisits'] + 1\n",
        "\n",
        "    # if completed, no need to continue to next move selection\n",
        "    if env.done:\n",
        "        if env.reward > 0:\n",
        "            curmemory['intrinsic'] = env.reward\n",
        "        return\n",
        "\n",
        "    # if not completed, continue to select next move\n",
        "    validmoves = env.getvalidmoves()\n",
        "    \n",
        "    # if no valid moves, no need to continue to next move selection\n",
        "    if validmoves == []:\n",
        "        return\n",
        "    \n",
        "    bestmove = None\n",
        "    bestvalue = -1e20\n",
        "    bestintrinsic = -1e20\n",
        "    \n",
        "    # choose best move\n",
        "    for move in validmoves:\n",
        "        newenv = copy.deepcopy(env)\n",
        "        newenv.step(move)\n",
        "        \n",
        "        nextmoves = newenv.numsteps\n",
        "        nextreward = newenv.reward\n",
        "        nextmemory = None\n",
        "        \n",
        "        nextstate = newenv.staterep()\n",
        "        if repeatedstate:\n",
        "            nextstate += str(statehistory.count(newenv.staterep())+1)\n",
        "        \n",
        "        if nextstate in memory:\n",
        "            nextmemory = memory[nextstate] \n",
        "            # update the nextmemory if agent has a better reward\n",
        "            if nextreward > nextmemory['reward']:\n",
        "                nextmemory['reward'] = nextreward\n",
        "                nextmemory['moves'] = curmoves + 1\n",
        "                nextmemory['numvisits'] = 0\n",
        "                nextmemory['actionhistory'] = actionhistory+[move]\n",
        "                nextmemory['statehistory'] = statehistory+[newenv.staterep()]\n",
        "                \n",
        "            # update the nextmemory if agent has similar reward but fewer number of moves\n",
        "            elif nextreward == nextmemory['reward'] and nextmemory['moves'] > curmoves + 1:\n",
        "                nextmemory['moves'] = curmoves + 1\n",
        "                nextmemory['numvisits'] = 0\n",
        "                nextmemory['actionhistory'] = actionhistory+[move]\n",
        "                nextmemory['statehistory'] = statehistory+[newenv.staterep()]\n",
        "                \n",
        "        # start a new memory if this is a new state\n",
        "        else:\n",
        "            # if no intrinsic guiding value, then do without intrinsic motivation\n",
        "            if intrinsic_fn is not None:\n",
        "                next_intrinsic_value = intrinsic_fn(newenv)\n",
        "            else:\n",
        "                next_intrinsic_value = 0\n",
        "            memory[nextstate] = {'statehistory': statehistory+[newenv.staterep()], 'reward': nextreward, 'intrinsic': next_intrinsic_value, 'moves': curmoves + 1, 'numvisits': 0, 'actionhistory': actionhistory+[move]}\n",
        "            nextmemory = memory[nextstate]\n",
        "            \n",
        "        # best intrinsic is the highest intrinsic value of all 1-step connections\n",
        "        bestintrinsic = max(bestintrinsic, nextmemory['intrinsic'])\n",
        "        \n",
        "        # determine the next square to visit via a heuristic\n",
        "        reward = nextmemory['reward'] \n",
        "        moves = nextmemory['moves'] \n",
        "        numvisits = nextmemory['numvisits']\n",
        "        intrinsic = nextmemory['intrinsic']\n",
        "        \n",
        "        totalvalue = reward_formula(reward = reward, intrinsic = intrinsic, moves = moves, numselected = 0, numvisits = numvisits)\n",
        "        \n",
        "        if totalvalue > bestvalue:\n",
        "            bestvalue = totalvalue\n",
        "            bestmove = move\n",
        "            \n",
        "        # update the one-step lookahead for intrinsic value\n",
        "        curmemory['intrinsic'] = bestintrinsic * 0.99\n",
        "    \n",
        "    return bestmove"
      ],
      "id": "88b2d20c-e678-40ad-8ede-930afe969646"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00e72846-7e64-4095-87f1-9dc62c60e161"
      },
      "source": [
        "# Helper Functions\n",
        "These functions help to perform hippocampal replay, and evaluation of agent on the environment.\n",
        "- MemoryReplay: Implements hippocampal replay\n",
        "- Game: Plays an environment for a single run (episode)\n",
        "- MultipleGame: Plays an environment for 100 runs"
      ],
      "id": "00e72846-7e64-4095-87f1-9dc62c60e161"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cef4c0eb-0b3f-4269-939e-4bf8098ae783"
      },
      "source": [
        "## Memory Replay"
      ],
      "id": "cef4c0eb-0b3f-4269-939e-4bf8098ae783"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "700ccfe8-a2e3-4269-a007-d5375b96359a"
      },
      "outputs": [],
      "source": [
        "''' Performs memory replay '''\n",
        "def MemoryReplay(env, bestactionhistory = [], agent = RandomAgent, maxsteps = 500, seed = None, **kwargs):\n",
        "    statehistory = []\n",
        "    actionhistory = []\n",
        "    statehistory.append(env.staterep())\n",
        "    historytuplelist = []\n",
        "    \n",
        "    # do forward replay\n",
        "    for move in bestactionhistory:\n",
        "        kwargs['statehistory'] = statehistory        \n",
        "        kwargs['actionhistory'] = actionhistory\n",
        "        kwargs['replay']=True\n",
        "        historytuplelist.append((copy.deepcopy(env), statehistory, actionhistory))\n",
        "        env.step(move)     \n",
        "        statehistory.append(env.staterep())\n",
        "        agent(copy.deepcopy(env), **kwargs)\n",
        "        actionhistory.append(move)\n",
        "    \n",
        "    # # do backward replay\n",
        "    backwardstates = []\n",
        "    for env, statehistory, actionhistory in historytuplelist[::-1]:\n",
        "        kwargs['statehistory'] = statehistory        \n",
        "        kwargs['actionhistory'] = actionhistory\n",
        "        kwargs['replay'] = True\n",
        "        agent(copy.deepcopy(env), **kwargs)\n",
        "        backwardstates.append(env.staterep())"
      ],
      "id": "700ccfe8-a2e3-4269-a007-d5375b96359a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d343a09-3981-4d88-8b96-f682b90fe176"
      },
      "source": [
        "## A Single Game"
      ],
      "id": "0d343a09-3981-4d88-8b96-f682b90fe176"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8be19f4d-94f4-4480-9b0f-53b716732409"
      },
      "outputs": [],
      "source": [
        "''' Plays 1 game '''\n",
        "def Game(env, agent = RandomAgent, actionhistory = [], statehistory = [], maxsteps = 500, seed = None, verbose = True, **kwargs):\n",
        "    \n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "    else:\n",
        "        np.random.seed(0)\n",
        "    while not env.done and env.numsteps < maxsteps:\n",
        "        statehistory.append(env.staterep())\n",
        "        kwargs['statehistory'] = statehistory        \n",
        "        kwargs['actionhistory'] = actionhistory\n",
        "        move = agent(env, **kwargs)\n",
        "        env.step(move)\n",
        "        actionhistory.append(move)\n",
        "            \n",
        "    statehistory.append(env.staterep())\n",
        "    kwargs['statehistory'] = statehistory\n",
        "    kwargs['actionhistory'] = actionhistory\n",
        "    # to update final state for RL agents\n",
        "    agent(env, **kwargs)\n",
        "    \n",
        "    if verbose:\n",
        "        print(env.done, env.reward, env.numsteps)\n",
        "        # env.print()\n",
        "    return env.done, env.reward, env.numsteps"
      ],
      "id": "8be19f4d-94f4-4480-9b0f-53b716732409"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c852e681-f332-48f4-ba3b-f2df2e8f5942"
      },
      "source": [
        "## Multiple Games"
      ],
      "id": "c852e681-f332-48f4-ba3b-f2df2e8f5942"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f7ffe41-7222-4308-8070-1c4f5a0e6b5f"
      },
      "outputs": [],
      "source": [
        "''' Plays multiple games '''\n",
        "def MultiGame(env, numtries = 100, hippocampal_replay = True, **kwargs):\n",
        "    solvedcount = 0\n",
        "    stephistory = []\n",
        "    bestmemory = 0\n",
        "    beststeps = 1000000\n",
        "    firstsolve = None\n",
        "    tries = numtries\n",
        "    solved = False\n",
        "    \n",
        "    for i in range(tries):\n",
        "        # choose a new state for GoExplore or GoExploreCount\n",
        "        if kwargs['agent'] in [GoExplore, GoExploreCount] and i > 0:\n",
        "            actionhistory, statehistory, nextenv = ChooseState(env = copy.deepcopy(env))\n",
        "            done, reward, steps = Game(seed = i, env = copy.deepcopy(nextenv), actionhistory = actionhistory+[], statehistory = statehistory+[], **kwargs)\n",
        "        else:\n",
        "            done, reward, steps = Game(seed = i, env = copy.deepcopy(env), actionhistory = [], statehistory = [], **kwargs)\n",
        "        if reward == 1:\n",
        "            solvedcount += 1\n",
        "            stephistory.append(steps)\n",
        "            \n",
        "            # if first solve, note how much memory is used\n",
        "            if solvedcount == 1:\n",
        "                bestmemory = len(memory)\n",
        "                firstsolve = i+1\n",
        "                \n",
        "            if hippocampal_replay:\n",
        "                # hippocampal replay only for goexplore or intrinsic agent\n",
        "                if kwargs['agent'] in [GoExplore, GoExploreCount, CountAgent]:\n",
        "                    actionhistory = None\n",
        "                    for key, value in memory.items():\n",
        "                        if memory[key]['reward'] == 1:\n",
        "                            actionhistory = memory[key]['actionhistory']\n",
        "\n",
        "                    # MemoryReplay to improve chance of optimal path being followed\n",
        "                    if actionhistory is not None:\n",
        "                        MemoryReplay(env = copy.deepcopy(env), bestactionhistory = actionhistory, **kwargs)\n",
        "    name = kwargs['agent'].__name__\n",
        "    if name == 'RandomAgent': \n",
        "        name = 'Random'\n",
        "        bestmemory = '-'\n",
        "    if name == 'QAgent': name = 'Q-Learning'\n",
        "    if name == 'TDAgent': name = 'TD-Learning'\n",
        "    if name == 'GoExplore': name = 'Go-Explore'\n",
        "    if name == 'GoExploreCount': name = 'Go-Explore-Count'\n",
        "    if name == 'CountAgent': name = 'Explore-Count'\n",
        "\n",
        "    if kwargs['agent'] == QAgent or kwargs['agent'] == TDAgent:\n",
        "        if kwargs.get('eps', 1) == 0:\n",
        "            name += ' (Test)'\n",
        "        else:\n",
        "            name += ' (Train)'\n",
        "            \n",
        "    if kwargs.get('intrinsic_fn', None) is not None:\n",
        "        name += ' GDIR'\n",
        "    if solvedcount == 0:\n",
        "        print(f\"{name} & {solvedcount}/{tries} & - & - & - & - & - \\\\\\\\\")\n",
        "    else:\n",
        "        print(f\"{name} & {solvedcount}/{tries} & {firstsolve} & {bestmemory} & {sum(stephistory)/len(stephistory):.1f} & {min(stephistory):.1f} & {max(stephistory):.1f} \\\\\\\\\")\n",
        "    # if solvedcount == 0:\n",
        "    #     print(f'Agent: {name}, No solves at all, Best Memory: {bestmemory}, Total Memory: {len(memory)}')\n",
        "    # else:\n",
        "    #     print(f'Agent: {name}, Solve rate: {solvedcount}/{tries} ({solvedcount/tries*100:.1f}%), First Solve: {firstsolve}, Best Memory: {bestmemory}, Steps: Avg {sum(stephistory)/len(stephistory):.1f}, Min {min(stephistory):.1f}, Max {max(stephistory):.1f}')"
      ],
      "id": "1f7ffe41-7222-4308-8070-1c4f5a0e6b5f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daca7925-e680-4bab-94d0-7392c8c79abe"
      },
      "source": [
        "# Discrete Environments 1 & 2 - Maze Environment (Unwalled, Walled)"
      ],
      "id": "daca7925-e680-4bab-94d0-7392c8c79abe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bfc749b-97af-48a2-a6ff-c4bdd1bd2eb7"
      },
      "outputs": [],
      "source": [
        "class MazeEnv:\n",
        "    def __init__(self, height=20, width=20, numbricks = 10, grid = None, doorpos = None, agentpos = None, randomseed = None):\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.doorpos = doorpos\n",
        "        self.agentpos = agentpos\n",
        "        self.numbricks = numbricks\n",
        "        self.randomseed = randomseed\n",
        "        self.numsteps = 0\n",
        "        self.done = False\n",
        "        self.reward = 0\n",
        "        if self.randomseed is not None:\n",
        "            np.random.seed(self.randomseed)\n",
        "        self.mapping = {0: '.', 1: 'X', 2: 'D', 3: '#'}\n",
        "        \n",
        "        # if grid not defined, do a random initialization of maze\n",
        "        if grid is None:\n",
        "            self.grid = np.zeros((self.height, self.width))\n",
        "            \n",
        "            # Step 1: get a door position that is valid\n",
        "            if doorpos is None:\n",
        "                self.doorpos = self.getvalidpos()\n",
        "            else:\n",
        "                self.doorpos = doorpos\n",
        "            self.grid[self.doorpos] = 2\n",
        "\n",
        "            # Step 2: get a start position that is valid\n",
        "            if agentpos is None:\n",
        "                self.agentpos = self.getvalidpos()\n",
        "            else:\n",
        "                self.agentpos = agentpos\n",
        "            self.grid[self.agentpos] = 1\n",
        "            \n",
        "            # Step 3: fill in the bricks\n",
        "            for i in range(self.numbricks):\n",
        "                self.grid[self.getvalidpos()] = 3\n",
        "                \n",
        "        # if grid predefined, get the parameters from there instead\n",
        "        else:\n",
        "            self.grid = grid\n",
        "            self.height, self.width = self.grid.shape\n",
        "            \n",
        "            lista, listb = np.where(self.grid == 2)\n",
        "            if len(lista) == 0 or len(listb) == 0:\n",
        "                self.doorpos = self.getvalidpos()\n",
        "            else:\n",
        "                self.doorpos = (lista[0], listb[0])\n",
        "            self.grid[self.doorpos] = 2\n",
        "            \n",
        "            lista, listb = np.where(self.grid == 1)\n",
        "            if len(lista) == 0 or len(listb) == 0:\n",
        "                self.agentpos = self.getvalidpos()\n",
        "            else:\n",
        "                self.agentpos = (lista[0], listb[0])\n",
        "            self.grid[self.agentpos] = 1\n",
        "            \n",
        "        # some variables to reset the environment\n",
        "        self.startgrid = self.grid.copy()\n",
        "        self.startagentpos = self.agentpos\n",
        "        self.startdoorpos = self.doorpos\n",
        "            \n",
        "    def reset(self):\n",
        "        self.grid = self.startgrid.copy()\n",
        "        self.agentpos = self.startagentpos\n",
        "        self.doorpos = self.startdoorpos\n",
        "        self.done = False\n",
        "        self.reward = 0\n",
        "        self.numsteps = 0\n",
        "        if self.randomseed is not None:\n",
        "            np.random.seed(self.randomseed)\n",
        "            \n",
        "    # gets state representation\n",
        "    def staterep(self):\n",
        "        return str(self.agentpos)\n",
        "            \n",
        "    # gets a valid position\n",
        "    def getvalidpos(self):\n",
        "        validpos = []\n",
        "        for i in range(self.height):\n",
        "            for j in range(self.width):\n",
        "                if self.grid[i,j] == 0:\n",
        "                    validpos.append((i,j))\n",
        "        return validpos[np.random.randint(len(validpos))]\n",
        "        \n",
        "    # checks if a position is valid that is not out of the grid and not occupied\n",
        "    def isvalid(self, pos, allowdoor = False):\n",
        "        if pos == None or len(pos)!=2:\n",
        "            return False\n",
        "        height, width = pos\n",
        "        if height < 0 or height >= self.height or width < 0 or width >= self.width:\n",
        "            return False\n",
        "        if allowdoor and self.grid[height,width] == 2:\n",
        "            return True\n",
        "        if self.grid[height,width] == 0:\n",
        "            return True\n",
        "        return False\n",
        "    \n",
        "    def step(self, move):\n",
        "        validmoves = self.getvalidmoves()\n",
        "        # randomly sample a move if not in validmoves\n",
        "        if move not in validmoves:\n",
        "            move = validmoves[np.random.randint(len(validmoves))]\n",
        "        self.numsteps += 1\n",
        "        self.grid[self.agentpos] = 0\n",
        "        self.agentpos = self.movedir(self.agentpos, move)\n",
        "        if self.agentpos == self.doorpos:\n",
        "            self.done = True\n",
        "            self.reward = 1\n",
        "        self.grid[self.agentpos] = 1\n",
        "    \n",
        "    def movedir(self, pos, d):\n",
        "        if pos == None or len(pos)!=2:\n",
        "            return False\n",
        "        height, width = pos\n",
        "        if d=='left':\n",
        "            return (height, width-1)\n",
        "        elif d=='right':\n",
        "            return (height, width+1)\n",
        "        elif d=='up':\n",
        "            return (height-1, width)\n",
        "        elif d=='down':\n",
        "            return (height+1, width)\n",
        "    \n",
        "    def getvalidmoves(self):\n",
        "        validmoves = []\n",
        "        for move in ['left', 'right', 'up', 'down']:\n",
        "            if self.isvalid(self.movedir(self.agentpos, move), allowdoor = True):\n",
        "                validmoves.append(move)\n",
        "        return validmoves\n",
        "    \n",
        "    def sample(self):\n",
        "        return np.random.choice(self.getvalidmoves())\n",
        "    \n",
        "    def print(self):\n",
        "        for i in range(self.height):\n",
        "            for j in range(self.width):\n",
        "                print(self.mapping[self.grid[i,j]], end = '')\n",
        "            print()"
      ],
      "id": "3bfc749b-97af-48a2-a6ff-c4bdd1bd2eb7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4907ded4-bf6b-4a23-8b2e-95dddf9f4e7b"
      },
      "outputs": [],
      "source": [
        "def Manhattan(env):\n",
        "    ''' Calculates the Manhattan distance between the agent and the door '''\n",
        "    pointA = env.agentpos\n",
        "    pointB = env.doorpos\n",
        "    return -(abs(pointA[0]-pointB[0])+abs(pointA[1]-pointB[1]))/(env.width+env.height-2)"
      ],
      "id": "4907ded4-bf6b-4a23-8b2e-95dddf9f4e7b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94c2cd90-ca17-4b85-8b1d-4993e73bfd83"
      },
      "source": [
        "## Unwalled maze (10x10)"
      ],
      "id": "94c2cd90-ca17-4b85-8b1d-4993e73bfd83"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55b99f46-6c75-401a-a4c4-4a05f15c7abe",
        "outputId": "728c9476-9d17-4945-8931-b0e412b3a58d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X.#...#...\n",
            "#..#......\n",
            "#.........\n",
            "........#.\n",
            "..........\n",
            "..........\n",
            ".........#\n",
            ".....#....\n",
            "#.....#...\n",
            ".........D\n"
          ]
        }
      ],
      "source": [
        "# This is how the maze looks like\n",
        "height, width = 10, 10\n",
        "env = MazeEnv(height = height, width = width, agentpos = (0, 0), doorpos = (height-1, width-1), randomseed = 1, numbricks = height*width//10)\n",
        "env.print()"
      ],
      "id": "55b99f46-6c75-401a-a4c4-4a05f15c7abe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c79753b3-35ee-4131-9a66-74a570eb1bc0",
        "outputId": "c4a4263b-4506-463b-bfd5-f3fecb9130f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random & 8/100 & 8 & - & 76.8 & 54.0 & 100.0 \\\\\n",
            "TD-Learning (Train) & 8/100 & 21 & 90 & 72.5 & 44.0 & 100.0 \\\\\n",
            "TD-Learning (Test) & 100/100 & 1 & 90 & 18.0 & 18.0 & 18.0 \\\\\n",
            "Q-Learning (Train) & 8/100 & 21 & 292 & 72.5 & 44.0 & 100.0 \\\\\n",
            "Q-Learning (Test) & 100/100 & 1 & 294 & 18.0 & 18.0 & 18.0 \\\\\n",
            "Go-Explore & 0/100 & - & - & - & - & - \\\\\n",
            "Go-Explore GDIR & 38/100 & 12 & 59 & 83.3 & 62.0 & 100.0 \\\\\n",
            "Go-Explore-Count & 100/100 & 1 & 71 & 62.0 & 62.0 & 62.0 \\\\\n",
            "Go-Explore-Count GDIR & 100/100 & 1 & 39 & 20.0 & 20.0 & 20.0 \\\\\n",
            "Explore-Count & 100/100 & 1 & 71 & 22.4 & 22.0 & 62.0 \\\\\n",
            "Explore-Count GDIR & 100/100 & 1 & 39 & 20.0 & 20.0 & 20.0 \\\\\n"
          ]
        }
      ],
      "source": [
        "for agent in [RandomAgent, TDAgent, QAgent]:\n",
        "    # print(agent.__name__, 'training')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False)\n",
        "    \n",
        "    # for QAgent and TDAgent, we include the final number of solves with deterministic transition\n",
        "    if agent == QAgent or agent == TDAgent:\n",
        "        # print(agent.__name__, 'testing')\n",
        "        MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, eps = 0)\n",
        "\n",
        "for agent in [GoExplore, GoExploreCount, CountAgent]:\n",
        "    # print(agent.__name__, 'without IM')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, intrinsic_fn = None)\n",
        "\n",
        "    # for Intrinsic Agent, we also do the run with intrinsic motivation\n",
        "    # print(agent.__name__, 'with IM')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, intrinsic_fn = Manhattan)"
      ],
      "id": "c79753b3-35ee-4131-9a66-74a570eb1bc0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3906a13-114b-4dfa-9330-298a1ecf5430"
      },
      "source": [
        "## Unwalled maze (20x20)"
      ],
      "id": "c3906a13-114b-4dfa-9330-298a1ecf5430"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fa0f838-6196-4dd3-a830-26ff49bbcc2e"
      },
      "outputs": [],
      "source": [
        "# This is how the maze looks like\n",
        "height, width = 20, 20\n",
        "env = MazeEnv(height = height, width = width, agentpos = (0, 0), doorpos = (height-1, width-1), randomseed = 1, numbricks = height*width//10)\n",
        "env.print()"
      ],
      "id": "5fa0f838-6196-4dd3-a830-26ff49bbcc2e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7129fb89-8201-4c0e-8e31-9ba26e7f3135"
      },
      "outputs": [],
      "source": [
        "for agent in [RandomAgent, TDAgent, QAgent]:\n",
        "    # print(agent.__name__, 'training')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False)\n",
        "    \n",
        "    # for QAgent and TDAgent, we include the final number of solves with deterministic transition\n",
        "    if agent == QAgent or agent == TDAgent:\n",
        "        # print(agent.__name__, 'testing')\n",
        "        MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, eps = 0)\n",
        "    \n",
        "for agent in [GoExplore, GoExploreCount, CountAgent]:\n",
        "    # print(agent.__name__, 'without IM')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, intrinsic_fn = None)\n",
        "\n",
        "    # for Intrinsic Agent, we also do the run with intrinsic motivation\n",
        "    # print(agent.__name__, 'with IM')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, intrinsic_fn = Manhattan)"
      ],
      "id": "7129fb89-8201-4c0e-8e31-9ba26e7f3135"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "422c28c5-f847-473d-98e2-f8d32902e78f"
      },
      "source": [
        "## Unwalled maze (100x100)"
      ],
      "id": "422c28c5-f847-473d-98e2-f8d32902e78f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c7d3007-7931-4ac0-9de6-9064d9c265ac",
        "outputId": "cb88e65f-9bb1-4957-b542-8a7e207ed1b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X..#............#...#.#...........##.........#..........#.........#........##........#........#...#.\n",
            ".......#.#...#.........#.................#...#...#...#..###...................#....#..#....#......#.\n",
            "....................................#...#.......#...............#...........#.....#.......#......#..\n",
            "...............#............#..#................#...........#..............................#.#.#....\n",
            ".......#...............#..........#.................................#..............#.....#..#.......\n",
            "#.........#.......#.#...........#.....#.#.....#..........#.......#..................................\n",
            "..........#...........#..#....................#.........#.#.............................#....#......\n",
            ".#...........#.##.........#.....................#........##.......#....................#....#...#...\n",
            ".....#............####..............#........#.#..##......#.......#.........................#.......\n",
            "....#..#.#..........#.......................#............#........#..................##.............\n",
            ".........#...........................#..................#.........#.......#.......##..#.....#..#....\n",
            "..#.............#............#............................#...#.......#..........................#..\n",
            "....#............#.....#.#...##..........#...#.#........................#............#..............\n",
            ".....#...........#.........#..................###.....#.........#.......#...#......#......#......#..\n",
            ".................#....#..........#............#..............#.........#................#...#.....#.\n",
            "#.....................#..#................##...........#...............#.....#.............#........\n",
            "....................#...#....#.#.#.........#............##........##..#...........................#.\n",
            ".......#.........#..............#........#....................#...............#....#...#...........#\n",
            "......................#.....#..............#.............#..............#..............#..#...#....#\n",
            "......#.......#................#.........................................##.........................\n",
            ".....#.................#.......#........#...#......#........#..........#......#.....................\n",
            "............................#.............#....#..#........##.........#.............................\n",
            ".......#......#.....#.......#.#..##..#......................#....#.....#..................##..#.....\n",
            ".....#...........#...................#.............#..#..#..............#.#.........................\n",
            "..........#.....#.#..............#..................................#...............................\n",
            ".................#..#....................#........#......#...#...#.....#..................#.........\n",
            ".....#.#....................#...#.....#...........#.#.........#.......#.............#.#.....##......\n",
            ".........#.....#..#....#.....#..............#................#......#..................#.....##.....\n",
            ".....#..........................................................#....#......#..........#..........#.\n",
            "..#...#.....#........#....#................#...........#..#........#........#.........#.........#...\n",
            "..........#......#.....................#.............#...#.........#.....#......##.......#.....#....\n",
            ".....#......#.....#......#...........#..........#...............#...................#...............\n",
            "........#..#..................................#.....##....#.................................#.......\n",
            ".............#...#...#......#..............##..#.#............#.........#.....#.....#...#.....#....#\n",
            ".......#......#.....#......#................#.....#......#..#......#.............#..................\n",
            "..............#................................#......................#.....#...#...................\n",
            "........#...#.......#.#....#.#.....................#.........#...#..................................\n",
            ".#.##......................................#......#..........#.#....................#.#.........##..\n",
            "........##........................#........................#........#...#.#.........................\n",
            "....#..#........#.......#......##.............................#..........#.......#.........#....#...\n",
            "...#......#....#....##.........#.........#.................................#....#...#.....#..#......\n",
            "....#.........#..................#......................#...........#.....#.......#...........#...##\n",
            "....#.........#............#..##...............##.........#.....#............#..............#.......\n",
            "........#..#...........#......................#.#....#...................................#.#........\n",
            "...........#.......#...........#........#.....#............#...............#.#......................\n",
            ".....................##.......#.....................#.....#............#.......#.........#..#..#....\n",
            "............#.........#...........#.....#..............#..#........##...........#...................\n",
            ".........#....#.#..#.....................................#...###.......##.#.........#.........#.....\n",
            "..............................#.#.#.#....#.................................#........................\n",
            "...............##.....#......#....#..#...#....#..#..#...........................................#...\n",
            ".#....#.......#....#.......................#......#.........#.........#..............#.#........#...\n",
            "#.#...........#..............###..#..#.................#...........#........#.............#...#.....\n",
            ".....#........#............#......#......#..........#..........#....#...............#..##.......#...\n",
            ".....#...#.......#.#........#.........#....#............#.......................#.###.....#.......#.\n",
            ".....#.#..#.#...#.........#.....#.........#..###.......#.#............................#.............\n",
            "#..................#..##.................#.#...#........................#......#.#............#.....\n",
            "..#...........#.........#.............................................##...........#........#..#....\n",
            ".....#...#................#....#.##......#.........#....#..................#....##...........#..#...\n",
            ".....#..........................#........#.....#.......#.............###......##..........#...#...#.\n",
            "...................#...#.#...................................#............#............#...........#\n",
            ".##..##.....#...........#..#.....................#.........#......................#..........#......\n",
            "..............#..........................##.................#......#..#.............................\n",
            "......#............#....#...#....................#..................#..............#................\n",
            ".....#.............#....................#.......#..#.....#.......................#...............#..\n",
            "...#......#..........#........#..............##....#...#..............#........#...........#..#....#\n",
            ".......#.................##....#.#...............#....#..#..#.................#.....................\n",
            "........#..........................................#.........................#..#...............#...\n",
            "........#.......##.......#....#......................##...#.................#..#...#..........#....#\n",
            "........#.......#........#......#.........#.#.#....#.#.......#.........#......#...................##\n",
            "#......#.................#........#..........................................................#......\n",
            "..#.......#...................#.....#..........#.....................#...#.#.......#...#.#..........\n",
            ".....#...............#...#............#...#.......#............#..............#.....................\n",
            "........................................................#.............##.........#..........#.#.##.#\n",
            ".....##.#.....##.#..............................................#....#......#...##......#...........\n",
            "...........#...............................#.....#.#...#.............#.....#.#.....#..............##\n",
            "..#..#...........#.......#............................#.................#..................#.....#..\n",
            "...........#.#..#............#.....#.#.#.........#........#.#.................##............#....#..\n",
            "#..#....................#..................##.#......#.....##......#...........#...#..#..........#..\n",
            ".............#...#.........#............#...........................#..........#...........#.....#..\n",
            ".#.............##.##......................#.....##.#....#....#........##...................#........\n",
            "...........#.#..#.....................#.#..#.............#..#.....#.......#.........................\n",
            ".....#....#.......##....#...........................##..........#......#....#....#..............#...\n",
            "..#..................#..........#.#...............#..#........#.....#....##.........#.#...#.........\n",
            "....##.........#....................#....#........#...#..................#.....#...#..#.#...........\n",
            "................#...............##.........................#......................#..........#.#....\n",
            "#....#....#.....#.#.......#.............................#...#.................................#..#..\n",
            ".................#...#..................#.#....................#.......#........##...##.............\n",
            ".................#...........#............#.............#..........#..........#...#..#......#.......\n",
            "....#.......#..........................#....##.....#..........#.......##...................#........\n",
            ".......#............#..#...........#...#.#........#..#...........................#..................\n",
            "##.........#...#.#..#.#................##...#......#.........#.#...........................#........\n",
            ".#............#.....................#...................................#..............#............\n",
            ".....#..#....#.#......#.......#....#..#.............#..................#....#...#...................\n",
            ".....................#..##....#............#.#.....#........................#.......................\n",
            ".....#..............#................#.......##....................#....................#..#.#...#..\n",
            "....#..........................#.#.....#.#........#...#.......#..#.............#....................\n",
            "#..........#.#....................................##..................#.............................\n",
            "....................#.......#.#..#..#.....#.#.......#...............#.#.........##..................\n",
            ".............#...................................#....#..#...#........##............#..#............\n",
            "..............#...........#.....................#..##........##.....#.........#....................D\n"
          ]
        }
      ],
      "source": [
        "# This is how the maze looks like\n",
        "height, width = 100, 100\n",
        "env = MazeEnv(height = height, width = width, agentpos = (0, 0), doorpos = (height-1, width-1), randomseed = 1, numbricks = height*width//10)\n",
        "env.print()"
      ],
      "id": "4c7d3007-7931-4ac0-9de6-9064d9c265ac"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "67f416d9-bec1-444d-863a-b893ef4b96ae",
        "outputId": "8be0c08b-dd90-4283-dc5c-f16ee63b688a"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-f9095b7577d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# print(agent.__name__, 'training')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mMultiGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumtries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# for QAgent and TDAgent, we include the final number of solves with deterministic transition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-4fba93026d71>\u001b[0m in \u001b[0;36mMultiGame\u001b[0;34m(env, numtries, hippocampal_replay, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnextenv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactionhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactionhistory\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatehistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatehistory\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactionhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatehistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0msolvedcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-bcec8320aee6>\u001b[0m in \u001b[0;36mGame\u001b[0;34m(env, agent, actionhistory, statehistory, maxsteps, seed, verbose, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'statehistory'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatehistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actionhistory'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactionhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mmove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mactionhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-6f901c1ef3d1>\u001b[0m in \u001b[0;36mRandomAgent\u001b[0;34m(env, **kwargs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mRandomAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-5a107f211f5b>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalidmoves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.randint\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m_bounded_integers.pyx\u001b[0m in \u001b[0;36mnumpy.random._bounded_integers._rand_int64\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mprod\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   3050\u001b[0m     \"\"\"\n\u001b[1;32m   3051\u001b[0m     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n\u001b[0;32m-> 3052\u001b[0;31m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[1;32m   3053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for agent in [RandomAgent, TDAgent, QAgent]:\n",
        "    # print(agent.__name__, 'training')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False)\n",
        "    \n",
        "    # for QAgent and TDAgent, we include the final number of solves with deterministic transition\n",
        "    if agent == QAgent or agent == TDAgent:\n",
        "        # print(agent.__name__, 'testing')\n",
        "        MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, eps = 0)\n",
        "    \n",
        "for agent in [GoExplore, GoExploreCount, CountAgent]:\n",
        "    # print(agent.__name__, 'without IM')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, intrinsic_fn = None)\n",
        "\n",
        "    # for Intrinsic Agent, we also do the run with intrinsic motivation\n",
        "    # print(agent.__name__, 'with IM')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, intrinsic_fn = Manhattan)"
      ],
      "id": "67f416d9-bec1-444d-863a-b893ef4b96ae"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7c81719-3778-495c-8f5c-31adc521b217"
      },
      "source": [
        "## Extra: Without hippocampal replay"
      ],
      "id": "a7c81719-3778-495c-8f5c-31adc521b217"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1046f337-03da-4abd-b157-97862221a2f0"
      },
      "outputs": [],
      "source": [
        "for agent in [RandomAgent, TDAgent, QAgent]:\n",
        "    # print(agent.__name__, 'training')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, hippocampal_replay = False, verbose = False)\n",
        "    \n",
        "    # for QAgent and TDAgent, we include the final number of solves with deterministic transition\n",
        "    if agent == QAgent or agent == TDAgent:\n",
        "        # print(agent.__name__, 'testing')\n",
        "        MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, hippocampal_replay = False, verbose = False, eps = 0)\n",
        "    \n",
        "for agent in [GoExplore, GoExploreCount, CountAgent]:\n",
        "    # print(agent.__name__, 'without IM')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, hippocampal_replay = False, verbose = False, intrinsic_fn = None)\n",
        "\n",
        "    # for Intrinsic Agent, we also do the run with intrinsic motivation\n",
        "    # print(agent.__name__, 'with IM')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, hippocampal_replay = False, verbose = False, intrinsic_fn = Manhattan)"
      ],
      "id": "1046f337-03da-4abd-b157-97862221a2f0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3b91b2c-2960-4b18-8fcd-808902e97a50"
      },
      "source": [
        "## Walled maze (10x10)"
      ],
      "id": "b3b91b2c-2960-4b18-8fcd-808902e97a50"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "644c7e52-d8f5-4309-a450-965af5e2f971"
      },
      "outputs": [],
      "source": [
        "# create game environment\n",
        "size = 10\n",
        "grid = np.zeros((size,size))\n",
        "maxheight, maxwidth = grid.shape\n",
        "grid[:, maxwidth//2-1] = 3\n",
        "grid[maxheight//2,:] = 3\n",
        "grid[1:maxheight, maxwidth-2] = 3\n",
        "grid[maxheight//2, maxwidth//4] = 0\n",
        "grid[maxheight//2, 3*maxwidth//4-1] = 0\n",
        "grid[3*maxheight//4, maxwidth//2-1] = 0\n",
        "grid[maxheight//2, maxwidth-1] = 0\n",
        "grid[0, 0] = 1\n",
        "grid[maxheight-1, maxwidth-1] = 2\n",
        "env = MazeEnv(grid = grid)\n",
        "env.print()"
      ],
      "id": "644c7e52-d8f5-4309-a450-965af5e2f971"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91f53743-83f1-46b5-bbea-b950e2738e52"
      },
      "outputs": [],
      "source": [
        "for agent in [RandomAgent, TDAgent, QAgent]:\n",
        "    # print(agent.__name__, 'training')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False)\n",
        "    \n",
        "    # for QAgent and TDAgent, we include the final number of solves with deterministic transition\n",
        "    if agent == QAgent or agent == TDAgent:\n",
        "        # print(agent.__name__, 'testing')\n",
        "        MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, eps = 0)\n",
        "    \n",
        "for agent in [GoExplore, GoExploreCount, CountAgent]:\n",
        "    # print(agent.__name__, 'without IM')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, intrinsic_fn = None)\n",
        "\n",
        "    # for Intrinsic Agent, we also do the run with intrinsic motivation\n",
        "    # print(agent.__name__, 'with IM')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, intrinsic_fn = Manhattan)"
      ],
      "id": "91f53743-83f1-46b5-bbea-b950e2738e52"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "103911ef-dd19-4146-8457-9116c2c352e6"
      },
      "source": [
        "## Walled maze (20x20)"
      ],
      "id": "103911ef-dd19-4146-8457-9116c2c352e6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b2f7559-5cc6-4556-8a8b-543cbe8e6cd4"
      },
      "outputs": [],
      "source": [
        "# create game environment\n",
        "size = 20\n",
        "grid = np.zeros((size,size))\n",
        "maxheight, maxwidth = grid.shape\n",
        "grid[:, maxwidth//2-1] = 3\n",
        "grid[maxheight//2,:] = 3\n",
        "grid[1:maxheight, maxwidth-2] = 3\n",
        "grid[maxheight//2, maxwidth//4] = 0\n",
        "grid[maxheight//2, 3*maxwidth//4-1] = 0\n",
        "grid[3*maxheight//4, maxwidth//2-1] = 0\n",
        "grid[maxheight//2, maxwidth-1] = 0\n",
        "grid[0, 0] = 1\n",
        "grid[maxheight-1, maxwidth-1] = 2\n",
        "env = MazeEnv(grid = grid)\n",
        "env.print()"
      ],
      "id": "3b2f7559-5cc6-4556-8a8b-543cbe8e6cd4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d3fdd4b-b771-4934-9d74-88c8639a6763"
      },
      "outputs": [],
      "source": [
        "for agent in [RandomAgent, TDAgent, QAgent]:\n",
        "    # print(agent.__name__, 'training')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False)\n",
        "    \n",
        "    # for QAgent and TDAgent, we include the final number of solves with deterministic transition\n",
        "    if agent == QAgent or agent == TDAgent:\n",
        "        # print(agent.__name__, 'testing')\n",
        "        MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, eps = 0)\n",
        "    \n",
        "for agent in [GoExplore, GoExploreCount, CountAgent]:\n",
        "    # print(agent.__name__, 'without IM')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, intrinsic_fn = None)\n",
        "\n",
        "    # for Intrinsic Agent, we also do the run with intrinsic motivation\n",
        "    # print(agent.__name__, 'with IM')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, intrinsic_fn = Manhattan)"
      ],
      "id": "6d3fdd4b-b771-4934-9d74-88c8639a6763"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "560e37e2-aaea-416b-8d9f-652da51aa602"
      },
      "source": [
        "## Walled maze (100x100)"
      ],
      "id": "560e37e2-aaea-416b-8d9f-652da51aa602"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9db7318b-2126-4ea0-96b6-9ef2715adc49",
        "outputId": "0c615854-ab45-4e3f-c7cb-1b5292484d00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X................................................#..................................................\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            "#########################.################################################.########################.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            "..................................................................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#.\n",
            ".................................................#................................................#D\n"
          ]
        }
      ],
      "source": [
        "# create game environment\n",
        "size = 100\n",
        "grid = np.zeros((size,size))\n",
        "maxheight, maxwidth = grid.shape\n",
        "grid[:, maxwidth//2-1] = 3\n",
        "grid[maxheight//2,:] = 3\n",
        "grid[1:maxheight, maxwidth-2] = 3\n",
        "grid[maxheight//2, maxwidth//4] = 0\n",
        "grid[maxheight//2, 3*maxwidth//4-1] = 0\n",
        "grid[3*maxheight//4, maxwidth//2-1] = 0\n",
        "grid[maxheight//2, maxwidth-1] = 0\n",
        "grid[0, 0] = 1\n",
        "grid[maxheight-1, maxwidth-1] = 2\n",
        "env = MazeEnv(grid = grid)\n",
        "env.print()"
      ],
      "id": "9db7318b-2126-4ea0-96b6-9ef2715adc49"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97c2936b-e682-4479-bbff-8657b3f2725d"
      },
      "outputs": [],
      "source": [
        "for agent in [RandomAgent, TDAgent, QAgent]:\n",
        "    # print(agent.__name__, 'training')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False)\n",
        "    \n",
        "    # for QAgent and TDAgent, we include the final number of solves with deterministic transition\n",
        "    if agent == QAgent or agent == TDAgent:\n",
        "        # print(agent.__name__, 'testing')\n",
        "        MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, eps = 0)\n",
        "    \n",
        "for agent in [GoExplore, GoExploreCount, CountAgent]:\n",
        "    # print(agent.__name__, 'without IM')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, intrinsic_fn = None)\n",
        "\n",
        "    # for Intrinsic Agent, we also do the run with intrinsic motivation\n",
        "    # print(agent.__name__, 'with IM')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, intrinsic_fn = Manhattan)"
      ],
      "id": "97c2936b-e682-4479-bbff-8657b3f2725d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8a45987-56db-40f1-bfb4-5f2e1aaa2005"
      },
      "source": [
        "## Extra: Without hippocampal replay"
      ],
      "id": "f8a45987-56db-40f1-bfb4-5f2e1aaa2005"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee761593-20e0-42ee-887c-f655e5f8a139"
      },
      "outputs": [],
      "source": [
        "for agent in [RandomAgent, TDAgent, QAgent]:\n",
        "    # print(agent.__name__, 'training')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, hippocampal_replay = False, verbose = False)\n",
        "    \n",
        "    # for QAgent and TDAgent, we include the final number of solves with deterministic transition\n",
        "    if agent == QAgent or agent == TDAgent:\n",
        "        # print(agent.__name__, 'testing')\n",
        "        MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, hippocampal_replay = False, verbose = False, eps = 0)\n",
        "    \n",
        "for agent in [GoExplore, GoExploreCount, CountAgent]:\n",
        "    # print(agent.__name__, 'without IM')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, hippocampal_replay = False, verbose = False, intrinsic_fn = None)\n",
        "\n",
        "    # for Intrinsic Agent, we also do the run with intrinsic motivation\n",
        "    # print(agent.__name__, 'with IM')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, hippocampal_replay = False, verbose = False, intrinsic_fn = Manhattan)"
      ],
      "id": "ee761593-20e0-42ee-887c-f655e5f8a139"
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "94c2cd90-ca17-4b85-8b1d-4993e73bfd83",
        "c3906a13-114b-4dfa-9330-298a1ecf5430",
        "422c28c5-f847-473d-98e2-f8d32902e78f",
        "a7c81719-3778-495c-8f5c-31adc521b217",
        "b3b91b2c-2960-4b18-8fcd-808902e97a50",
        "103911ef-dd19-4146-8457-9116c2c352e6",
        "6c369ecf-3c18-4f5f-b903-41a5ac270ce3",
        "04684dda-9750-4e4c-8502-4d18e10f4c70",
        "5ba22122-fe33-4040-945d-c5c9347e4d9f",
        "9b0a5c93-092c-4c79-aca2-1ed615fa9e33",
        "d7205b46-aae5-4cce-b62b-edd9a055a87f",
        "49fbcf92-f9d3-48e5-b674-23c167736ee4",
        "f03919d4-7e0d-427e-9fd1-83b285c73ecc",
        "a53832de-0192-49d1-b393-90dbc0248161",
        "839dad9d-c63b-465d-9a42-519b036bf5e0",
        "56ccdbb7-44e2-4967-b303-5c0316c5fd0f",
        "3bb96474-4be3-48dd-ae4f-65a630ae3ee7",
        "705ed15a-9dba-45cd-a83a-d08b549dcad6",
        "3389dbdf-6d9b-47dc-9edf-c1c63bd42429",
        "6867989e-781a-4739-8030-fff41434df51"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}