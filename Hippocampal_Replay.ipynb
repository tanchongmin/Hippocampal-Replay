{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b7ed041-a82b-4e7d-9e4f-a8efcae2a7ee"
      },
      "source": [
        "# Introduction\n",
        "This notebook contains the 4 (+2 bonus) agents used in the paper \"Using Hippocampal Replay to Consolidate Experiences in Memory-Augmented Reinforcement Learning\"\n",
        "- Random\n",
        "- TD (bonus)\n",
        "- Q-Learning (bonus)\n",
        "- Go-Explore\n",
        "- Go-Explore-Count\n",
        "- Explore-Count\n",
        "\n",
        "It contains the 2 discrete state environments used\n",
        "- Unwalled Maze\n",
        "- Walled Maze"
      ],
      "id": "3b7ed041-a82b-4e7d-9e4f-a8efcae2a7ee"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "000f59b7-0167-4400-a116-1e9fcb2544e0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "from collections import defaultdict"
      ],
      "id": "000f59b7-0167-4400-a116-1e9fcb2544e0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d61ceb7c-eac3-4cfe-a31e-356f0582d624"
      },
      "source": [
        "# Agent Definition"
      ],
      "id": "d61ceb7c-eac3-4cfe-a31e-356f0582d624"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93ca1159-6106-4f25-bf57-1643e64a5781"
      },
      "outputs": [],
      "source": [
        "# this is the memory for the agents which need them\n",
        "memory = defaultdict(lambda: 0)"
      ],
      "id": "93ca1159-6106-4f25-bf57-1643e64a5781"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38ab6955-10f1-4752-acf9-d6436efedf0c"
      },
      "source": [
        "## Agent 1: Random Agent"
      ],
      "id": "38ab6955-10f1-4752-acf9-d6436efedf0c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "545a9b0d-37ce-4375-a410-3936e18da3ce"
      },
      "outputs": [],
      "source": [
        "def RandomAgent(env, **kwargs):\n",
        "    return env.sample()"
      ],
      "id": "545a9b0d-37ce-4375-a410-3936e18da3ce"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6c7bbc6-773e-4cb0-b42f-0eb2cda714a6"
      },
      "source": [
        "## Agent 2: TD Agent"
      ],
      "id": "f6c7bbc6-773e-4cb0-b42f-0eb2cda714a6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b4fc117-756b-41c0-b5e2-89eead91104a"
      },
      "source": [
        "TD-error: $\\delta_t = r_{t} + \\gamma \\max_{a\\in A, a: s_t \\rightarrow s_{t+1}}V(s_{t+1}) - V(s_t)$\n",
        "\n",
        "Value update: $V(s_t) \\leftarrow V(s_t) + \\alpha\\delta_t$"
      ],
      "id": "2b4fc117-756b-41c0-b5e2-89eead91104a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67d91115-7ead-408d-8699-a09167c70a15"
      },
      "outputs": [],
      "source": [
        "def TDAgent(env, **kwargs):\n",
        "    eps = kwargs.get('eps', 1)\n",
        "    GAMMA = 0.99\n",
        "    ALPHA = 1\n",
        "    \n",
        "    statehistory = kwargs.get('statehistory', [])\n",
        "    repeatedstate = kwargs.get('repeatedstate', False)\n",
        "    \n",
        "    curstate = env.staterep()\n",
        "    if repeatedstate:\n",
        "        curstate += str(statehistory.count(env.staterep()))\n",
        "    \n",
        "    if env.reward == 1:\n",
        "        memory[curstate] = 1\n",
        "        return\n",
        "\n",
        "    validmoves = env.getvalidmoves()\n",
        "    bestmove = None\n",
        "    bestvalue = -1\n",
        "    \n",
        "    # choose best move\n",
        "    for move in validmoves:\n",
        "        newenv = copy.deepcopy(env)\n",
        "        newenv.step(move)\n",
        "        nextstate = newenv.staterep()\n",
        "        if repeatedstate:\n",
        "            nextstate += str(statehistory.count(newenv.staterep())+1)\n",
        "            \n",
        "        curvalue = memory[nextstate] \n",
        "        if curvalue > bestvalue:\n",
        "            bestvalue = curvalue\n",
        "            bestmove = move\n",
        "    \n",
        "    # if epsilon, then choose randomly\n",
        "    if eps > np.random.rand():\n",
        "        bestmove = env.sample()\n",
        "        \n",
        "    # update current state value with optimal one-step lookahead estimate\n",
        "    td_error = env.reward + GAMMA*bestvalue - memory[curstate]\n",
        "    memory[curstate] = memory[curstate] + ALPHA*td_error\n",
        "    \n",
        "    return bestmove"
      ],
      "id": "67d91115-7ead-408d-8699-a09167c70a15"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54cf064e-898f-445d-8ed9-4e83cc98ffb7"
      },
      "source": [
        "## Agent 3: Q-Learning Agent"
      ],
      "id": "54cf064e-898f-445d-8ed9-4e83cc98ffb7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d760d1c-eec6-4721-bc42-294588e6d1fc"
      },
      "source": [
        "TD-error: $\\delta_t = r_t + \\gamma\\max_{a\\in A}Q(s_{t+1},a) - Q(s_t, a_t)$\n",
        "\n",
        "Q-learning update: $Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha\\delta_t$"
      ],
      "id": "5d760d1c-eec6-4721-bc42-294588e6d1fc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2357a12a-ad42-43b3-8909-e17f4159be69"
      },
      "outputs": [],
      "source": [
        "def QAgent(env, **kwargs):\n",
        "    eps = kwargs.get('eps', 1)\n",
        "    statehistory = kwargs.get('statehistory', [])\n",
        "    repeatedstate = kwargs.get('repeatedstate', False)\n",
        "    \n",
        "    GAMMA = 0.99\n",
        "    ALPHA = 1\n",
        "    \n",
        "    curstate = env.staterep()\n",
        "    if repeatedstate:\n",
        "        curstate += str(statehistory.count(env.staterep()))\n",
        "    \n",
        "    if env.reward == 1:\n",
        "        for move in env.getvalidmoves():\n",
        "            memory[(curstate, move)] = 1\n",
        "        return\n",
        "\n",
        "    validmoves = env.getvalidmoves()\n",
        "    bestmove = None\n",
        "    bestvalue = -1\n",
        "    \n",
        "    # if epsilon, then choose randomly\n",
        "    if eps > np.random.rand():\n",
        "        bestmove = env.sample()\n",
        "    # else choose best move\n",
        "    else:\n",
        "        for move in validmoves:\n",
        "            curvalue = memory[(curstate, move)] \n",
        "            if curvalue > bestvalue:\n",
        "                bestvalue = curvalue\n",
        "                bestmove = move\n",
        "        \n",
        "    # do a one-step in the next direction\n",
        "    newenv = copy.deepcopy(env)\n",
        "    newenv.step(bestmove)\n",
        "    nextstate = newenv.staterep()\n",
        "    if repeatedstate:\n",
        "        nextstate += str(statehistory.count(newenv.staterep())+1)\n",
        "    td_error = env.reward + GAMMA*np.max([memory[(nextstate, move)] for move in newenv.getvalidmoves()]) - memory[(curstate, bestmove)]\n",
        "        \n",
        "    # update the Q-function\n",
        "    memory[(curstate, bestmove)] = memory[(curstate, bestmove)] + ALPHA*td_error\n",
        "    \n",
        "    return bestmove"
      ],
      "id": "2357a12a-ad42-43b3-8909-e17f4159be69"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "934f2d62-11b0-4150-8a76-14039e20c963"
      },
      "source": [
        "## Agent 4: Go-Explore"
      ],
      "id": "934f2d62-11b0-4150-8a76-14039e20c963"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59265644-3b53-4894-8651-7e34b9b5d1cc"
      },
      "outputs": [],
      "source": [
        "def reward_formula(reward = 0, intrinsic = 0, moves = 0, numselected = 0, numvisits = 0, eps = 1e-20):\n",
        "    return 1000*reward + 10*intrinsic + 1*np.sqrt(moves) - 100*np.sqrt(numselected+numvisits)"
      ],
      "id": "59265644-3b53-4894-8651-7e34b9b5d1cc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cbb6722-3ca7-43be-870a-8b201122320b"
      },
      "outputs": [],
      "source": [
        "''' Chooses the next best memory state to go to '''\n",
        "def ChooseState(env):\n",
        "    bestvalue = -1e20\n",
        "    bestkey = None\n",
        "    \n",
        "    # choose the best memory based on heuristics\n",
        "    for key in memory:\n",
        "        # do not choose final state as there is nothing left to explore\n",
        "        if memory[key]['reward'] == 1: \n",
        "            continue\n",
        "        \n",
        "        curmem = memory[key]\n",
        "        reward = curmem['reward']\n",
        "        intrinsic = curmem['intrinsic']\n",
        "        moves = curmem['moves']\n",
        "        numselected = curmem.get('numselected', 0)\n",
        "        numvisits = curmem['numvisits']\n",
        "        \n",
        "        value = reward_formula(reward = reward, intrinsic = intrinsic, moves = moves, numselected = numselected, numvisits = numvisits)\n",
        "        if value > bestvalue:\n",
        "            bestvalue = value\n",
        "            bestkey = key\n",
        "    \n",
        "    # generate the trajectory to get the environment state\n",
        "    actionhistory = []\n",
        "    statehistory = []\n",
        "    \n",
        "    for move in memory[bestkey]['actionhistory']:\n",
        "        statehistory.append(env.staterep())\n",
        "        env.step(move)\n",
        "        actionhistory.append(move)\n",
        "        \n",
        "    # increment the selection visit count\n",
        "    if 'numselected' in memory[bestkey]:\n",
        "        memory[bestkey]['numselected'] = memory[bestkey]['numselected'] + 1\n",
        "    \n",
        "    return (actionhistory, statehistory, copy.deepcopy(env))\n",
        "        "
      ],
      "id": "3cbb6722-3ca7-43be-870a-8b201122320b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "897adc24-76f0-4c3f-8e4b-cd8b6cffd26e"
      },
      "outputs": [],
      "source": [
        "''' Chooses the best move based on memory and intrinsic rewards '''\n",
        "def GoExplore(env, **kwargs):\n",
        "    \n",
        "    intrinsic_fn = kwargs.get('intrinsic_fn', None)\n",
        "    replay = kwargs.get('replay', False)\n",
        "    getbestmove = kwargs.get('getbestmove', False)\n",
        "    statehistory = kwargs.get('statehistory', [])\n",
        "    actionhistory = kwargs.get('actionhistory', [])\n",
        "    repeatedstate = kwargs.get('repeatedstate', False)\n",
        "    \n",
        "    # if no intrinsic guiding value, then do without intrinsic motivation\n",
        "    if intrinsic_fn is not None:\n",
        "        intrinsic_value = intrinsic_fn(env)\n",
        "    else:\n",
        "        intrinsic_value = 0\n",
        "        \n",
        "    curmoves = env.numsteps\n",
        "    curreward = env.reward\n",
        "\n",
        "    curstate = env.staterep()\n",
        "    if repeatedstate:\n",
        "        curstate += str(statehistory.count(env.staterep()))\n",
        "        \n",
        "    # if this state is not present in memory (should only happen for start state), add it in\n",
        "    if curstate not in memory:\n",
        "        memory[curstate] = {'statehistory': statehistory+[], 'reward': curreward, 'intrinsic': intrinsic_value, 'moves': curmoves, 'numvisits': 0, 'numselected': 0, 'actionhistory': actionhistory+[]}\n",
        "        \n",
        "    curmemory = memory[curstate]\n",
        "    \n",
        "    # only increment memory if not doing replay\n",
        "    if replay:\n",
        "        curmemory['numvisits'] = 0\n",
        "        curmemory['numselected'] = 0\n",
        "    else:\n",
        "        curmemory['numvisits'] = curmemory['numvisits'] + 1\n",
        "    \n",
        "    # if completed, no need to continue to next move selection\n",
        "    if env.done:\n",
        "        # if there is positive reward, then make intrinsic become extrinsic reward\n",
        "        if env.reward > 0 and intrinsic_fn is not None:\n",
        "            curmemory['intrinsic'] = env.reward\n",
        "        return\n",
        "\n",
        "    # if not completed, continue to select next move\n",
        "    validmoves = env.getvalidmoves()\n",
        "    \n",
        "    # if no valid moves, no need to continue to next move selection\n",
        "    if validmoves == []:\n",
        "        return\n",
        "    \n",
        "    bestmove = None\n",
        "    bestvalue = -1e20\n",
        "    bestintrinsic = -1e20\n",
        "    \n",
        "    # choose best move\n",
        "    for move in validmoves:\n",
        "        newenv = copy.deepcopy(env)\n",
        "        newenv.step(move)\n",
        "        \n",
        "        nextmoves = newenv.numsteps\n",
        "        nextreward = newenv.reward\n",
        "        nextmemory = None\n",
        "        \n",
        "        nextstate = newenv.staterep()\n",
        "        if repeatedstate:\n",
        "            nextstate += str(statehistory.count(newenv.staterep())+1)\n",
        "        \n",
        "        if nextstate in memory:\n",
        "            nextmemory = memory[nextstate] \n",
        "            # update the nextmemory if agent has a better reward\n",
        "            if nextreward > nextmemory['reward']:\n",
        "                nextmemory['reward'] = nextreward\n",
        "                nextmemory['moves'] = curmoves + 1\n",
        "                nextmemory['numvisits'] = 0\n",
        "                nextmemory['numselected'] = 0\n",
        "                nextmemory['actionhistory'] = actionhistory+[move]\n",
        "                nextmemory['statehistory'] = statehistory+[newenv.staterep()]\n",
        "                \n",
        "            # update the nextmemory if agent has similar reward but fewer number of moves\n",
        "            elif nextreward == nextmemory['reward'] and nextmemory['moves'] > curmoves + 1:\n",
        "                nextmemory['moves'] = curmoves + 1\n",
        "                nextmemory['numvisits'] = 0\n",
        "                nextmemory['numselected'] = 0\n",
        "                nextmemory['actionhistory'] = actionhistory+[move]\n",
        "                nextmemory['statehistory'] = statehistory+[newenv.staterep()]\n",
        "                \n",
        "        # start a new memory if this is a new state\n",
        "        else:\n",
        "            # if no intrinsic guiding value, then do without intrinsic motivation\n",
        "            if intrinsic_fn is not None:\n",
        "                next_intrinsic_value = intrinsic_fn(newenv)\n",
        "            else:\n",
        "                next_intrinsic_value = 0\n",
        "            memory[nextstate] = {'statehistory': statehistory+[newenv.staterep()], 'reward': nextreward, 'intrinsic': next_intrinsic_value, 'moves': curmoves + 1, 'numvisits': 0, 'numselected': 0, 'actionhistory': actionhistory+[move]}\n",
        "            nextmemory = memory[nextstate]\n",
        "                \n",
        "        # best intrinsic is the highest intrinsic value of all 1-step connections\n",
        "        bestintrinsic = max(bestintrinsic, nextmemory['intrinsic'])\n",
        "        \n",
        "        # determine the next square to visit via a heuristic\n",
        "        reward = nextmemory['reward'] \n",
        "        moves = nextmemory['moves'] \n",
        "        numvisits = nextmemory['numvisits']\n",
        "        intrinsic = nextmemory['intrinsic']\n",
        "        \n",
        "        totalvalue = reward_formula(reward = reward, intrinsic = intrinsic, moves = moves, numselected = 0, numvisits = numvisits)\n",
        "      \n",
        "        if totalvalue > bestvalue or bestvalue is None:\n",
        "            bestvalue = totalvalue\n",
        "            bestmove = move\n",
        "            \n",
        "    # update the one-step lookahead for intrinsic value\n",
        "    curmemory['intrinsic'] = bestintrinsic*0.99\n",
        "    \n",
        "    if getbestmove:\n",
        "        return bestmove\n",
        "    else:\n",
        "        return np.random.choice(validmoves)"
      ],
      "id": "897adc24-76f0-4c3f-8e4b-cd8b6cffd26e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9771605f-ad3f-413b-8680-196b06bb2fae"
      },
      "outputs": [],
      "source": [
        "''' Chooses Best Move '''\n",
        "def GoExploreCount(env, **kwargs):\n",
        "    return GoExplore(env, getbestmove = True, **kwargs)"
      ],
      "id": "9771605f-ad3f-413b-8680-196b06bb2fae"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d04090b3-345e-471b-ad65-264d1f070865"
      },
      "source": [
        "## Agent 5: Count Agent"
      ],
      "id": "d04090b3-345e-471b-ad65-264d1f070865"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88b2d20c-e678-40ad-8ede-930afe969646"
      },
      "outputs": [],
      "source": [
        "''' Chooses the best move based on memory and intrinsic rewards '''\n",
        "def CountAgent(env, **kwargs):\n",
        "    \n",
        "    intrinsic_fn = kwargs.get('intrinsic_fn', None)\n",
        "    replay = kwargs.get('replay', False)\n",
        "    statehistory = kwargs.get('statehistory', [])\n",
        "    actionhistory = kwargs.get('actionhistory', [])\n",
        "    repeatedstate = kwargs.get('repeatedstate', False)\n",
        "    \n",
        "    # if no intrinsic guiding value, then do without intrinsic motivation\n",
        "    if intrinsic_fn is not None:\n",
        "        intrinsic_value = intrinsic_fn(env)\n",
        "    else:\n",
        "        intrinsic_value = 0\n",
        "        \n",
        "    curmoves = env.numsteps\n",
        "    curreward = env.reward\n",
        "\n",
        "    curstate = env.staterep()\n",
        "    if repeatedstate:\n",
        "        curstate += str(statehistory.count(env.staterep()))\n",
        "\n",
        "    # if this state is not present in memory (should only happen for start state), add it in\n",
        "    if curstate not in memory:\n",
        "        memory[curstate] = {'statehistory': statehistory+[], 'reward': curreward, 'intrinsic': intrinsic_value, 'moves': curmoves, 'numvisits': 0, 'actionhistory': actionhistory+[]}\n",
        "        \n",
        "    curmemory = memory[curstate]\n",
        "    \n",
        "    # only increment memory if not doing replay\n",
        "    if replay:\n",
        "        curmemory['numvisits'] = 0\n",
        "    else:\n",
        "        curmemory['numvisits'] = curmemory['numvisits'] + 1\n",
        "\n",
        "    # if completed, no need to continue to next move selection\n",
        "    if env.done:\n",
        "        if env.reward > 0 and intrinsic_fn is not None:\n",
        "            curmemory['intrinsic'] = env.reward\n",
        "        return\n",
        "\n",
        "    # if not completed, continue to select next move\n",
        "    validmoves = env.getvalidmoves()\n",
        "    \n",
        "    # if no valid moves, no need to continue to next move selection\n",
        "    if validmoves == []:\n",
        "        return\n",
        "    \n",
        "    bestmove = None\n",
        "    bestvalue = -1e20\n",
        "    bestintrinsic = -1e20\n",
        "    \n",
        "    # choose best move\n",
        "    for move in validmoves:\n",
        "        newenv = copy.deepcopy(env)\n",
        "        newenv.step(move)\n",
        "        \n",
        "        nextmoves = newenv.numsteps\n",
        "        nextreward = newenv.reward\n",
        "        nextmemory = None\n",
        "        \n",
        "        nextstate = newenv.staterep()\n",
        "        if repeatedstate:\n",
        "            nextstate += str(statehistory.count(newenv.staterep())+1)\n",
        "        \n",
        "        if nextstate in memory:\n",
        "            nextmemory = memory[nextstate] \n",
        "            # update the nextmemory if agent has a better reward\n",
        "            if nextreward > nextmemory['reward']:\n",
        "                nextmemory['reward'] = nextreward\n",
        "                nextmemory['moves'] = curmoves + 1\n",
        "                nextmemory['numvisits'] = 0\n",
        "                nextmemory['actionhistory'] = actionhistory+[move]\n",
        "                nextmemory['statehistory'] = statehistory+[newenv.staterep()]\n",
        "                \n",
        "            # update the nextmemory if agent has similar reward but fewer number of moves\n",
        "            elif nextreward == nextmemory['reward'] and nextmemory['moves'] > curmoves + 1:\n",
        "                nextmemory['moves'] = curmoves + 1\n",
        "                nextmemory['numvisits'] = 0\n",
        "                nextmemory['actionhistory'] = actionhistory+[move]\n",
        "                nextmemory['statehistory'] = statehistory+[newenv.staterep()]\n",
        "                \n",
        "        # start a new memory if this is a new state\n",
        "        else:\n",
        "            # if no intrinsic guiding value, then do without intrinsic motivation\n",
        "            if intrinsic_fn is not None:\n",
        "                next_intrinsic_value = intrinsic_fn(newenv)\n",
        "            else:\n",
        "                next_intrinsic_value = 0\n",
        "            memory[nextstate] = {'statehistory': statehistory+[newenv.staterep()], 'reward': nextreward, 'intrinsic': next_intrinsic_value, 'moves': curmoves + 1, 'numvisits': 0, 'actionhistory': actionhistory+[move]}\n",
        "            nextmemory = memory[nextstate]\n",
        "            \n",
        "        # best intrinsic is the highest intrinsic value of all 1-step connections\n",
        "        bestintrinsic = max(bestintrinsic, nextmemory['intrinsic'])\n",
        "        \n",
        "        # determine the next square to visit via a heuristic\n",
        "        reward = nextmemory['reward'] \n",
        "        moves = nextmemory['moves'] \n",
        "        numvisits = nextmemory['numvisits']\n",
        "        intrinsic = nextmemory['intrinsic']\n",
        "        \n",
        "        totalvalue = reward_formula(reward = reward, intrinsic = intrinsic, moves = moves, numselected = 0, numvisits = numvisits)\n",
        "        \n",
        "        if totalvalue > bestvalue:\n",
        "            bestvalue = totalvalue\n",
        "            bestmove = move\n",
        "            \n",
        "        # update the one-step lookahead for intrinsic value\n",
        "        curmemory['intrinsic'] = bestintrinsic * 0.99\n",
        "    \n",
        "    return bestmove"
      ],
      "id": "88b2d20c-e678-40ad-8ede-930afe969646"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00e72846-7e64-4095-87f1-9dc62c60e161"
      },
      "source": [
        "# Helper Functions\n",
        "These functions help to perform hippocampal replay, and evaluation of agent on the environment.\n",
        "- MemoryReplay: Implements hippocampal replay\n",
        "- Game: Plays an environment for a single run (episode)\n",
        "- MultipleGame: Plays an environment for 100 runs"
      ],
      "id": "00e72846-7e64-4095-87f1-9dc62c60e161"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cef4c0eb-0b3f-4269-939e-4bf8098ae783"
      },
      "source": [
        "## Memory Replay"
      ],
      "id": "cef4c0eb-0b3f-4269-939e-4bf8098ae783"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "700ccfe8-a2e3-4269-a007-d5375b96359a"
      },
      "outputs": [],
      "source": [
        "''' Performs memory replay '''\n",
        "def MemoryReplay(env, bestactionhistory = [], agent = RandomAgent, maxsteps = 500, seed = None, **kwargs):\n",
        "    statehistory = []\n",
        "    actionhistory = []\n",
        "    statehistory.append(env.staterep())\n",
        "    historytuplelist = []\n",
        "    \n",
        "    # do forward replay\n",
        "    for move in bestactionhistory:\n",
        "        kwargs['statehistory'] = statehistory        \n",
        "        kwargs['actionhistory'] = actionhistory\n",
        "        kwargs['replay']=True\n",
        "        historytuplelist.append((copy.deepcopy(env), statehistory, actionhistory))\n",
        "        env.step(move)     \n",
        "        statehistory.append(env.staterep())\n",
        "        agent(copy.deepcopy(env), **kwargs)\n",
        "        actionhistory.append(move)\n",
        "    \n",
        "    # # do backward replay\n",
        "    backwardstates = []\n",
        "    for env, statehistory, actionhistory in historytuplelist[::-1]:\n",
        "        kwargs['statehistory'] = statehistory        \n",
        "        kwargs['actionhistory'] = actionhistory\n",
        "        kwargs['replay'] = True\n",
        "        agent(copy.deepcopy(env), **kwargs)\n",
        "        backwardstates.append(env.staterep())"
      ],
      "id": "700ccfe8-a2e3-4269-a007-d5375b96359a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d343a09-3981-4d88-8b96-f682b90fe176"
      },
      "source": [
        "## A Single Game"
      ],
      "id": "0d343a09-3981-4d88-8b96-f682b90fe176"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8be19f4d-94f4-4480-9b0f-53b716732409"
      },
      "outputs": [],
      "source": [
        "''' Plays 1 game '''\n",
        "def Game(env, agent = RandomAgent, actionhistory = [], statehistory = [], maxsteps = 500, seed = None, verbose = True, **kwargs):\n",
        "    \n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "    else:\n",
        "        np.random.seed(0)\n",
        "    while not env.done and env.numsteps < maxsteps:\n",
        "        statehistory.append(env.staterep())\n",
        "        kwargs['statehistory'] = statehistory        \n",
        "        kwargs['actionhistory'] = actionhistory\n",
        "        move = agent(env, **kwargs)\n",
        "        env.step(move)\n",
        "        actionhistory.append(move)\n",
        "            \n",
        "    statehistory.append(env.staterep())\n",
        "    kwargs['statehistory'] = statehistory\n",
        "    kwargs['actionhistory'] = actionhistory\n",
        "    # to update final state for RL agents\n",
        "    agent(env, **kwargs)\n",
        "    \n",
        "    if verbose:\n",
        "        print(env.done, env.reward, env.numsteps)\n",
        "        # env.print()\n",
        "    return env.done, env.reward, env.numsteps"
      ],
      "id": "8be19f4d-94f4-4480-9b0f-53b716732409"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c852e681-f332-48f4-ba3b-f2df2e8f5942"
      },
      "source": [
        "## Multiple Games"
      ],
      "id": "c852e681-f332-48f4-ba3b-f2df2e8f5942"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f7ffe41-7222-4308-8070-1c4f5a0e6b5f"
      },
      "outputs": [],
      "source": [
        "''' Plays multiple games '''\n",
        "def MultiGame(env, numtries = 100, hippocampal_replay = True, **kwargs):\n",
        "    solvedcount = 0\n",
        "    stephistory = []\n",
        "    bestmemory = 0\n",
        "    beststeps = 1000000\n",
        "    firstsolve = None\n",
        "    tries = numtries\n",
        "    solved = False\n",
        "    \n",
        "    for i in range(tries):\n",
        "        # choose a new state for GoExplore or GoExploreCount\n",
        "        if kwargs['agent'] in [GoExplore, GoExploreCount] and i > 0:\n",
        "            actionhistory, statehistory, nextenv = ChooseState(env = copy.deepcopy(env))\n",
        "            done, reward, steps = Game(seed = i, env = copy.deepcopy(nextenv), actionhistory = actionhistory+[], statehistory = statehistory+[], **kwargs)\n",
        "        else:\n",
        "            done, reward, steps = Game(seed = i, env = copy.deepcopy(env), actionhistory = [], statehistory = [], **kwargs)\n",
        "        if reward == 1:\n",
        "            solvedcount += 1\n",
        "            stephistory.append(steps)\n",
        "            \n",
        "            # if first solve, note how much memory is used\n",
        "            if solvedcount == 1:\n",
        "                bestmemory = len(memory)\n",
        "                firstsolve = i+1\n",
        "                \n",
        "            if hippocampal_replay:\n",
        "                # hippocampal replay only for goexplore or intrinsic agent\n",
        "                if kwargs['agent'] in [GoExplore, GoExploreCount, CountAgent]:\n",
        "                    actionhistory = None\n",
        "                    for key, value in memory.items():\n",
        "                        if memory[key]['reward'] == 1:\n",
        "                            actionhistory = memory[key]['actionhistory']\n",
        "\n",
        "                    # MemoryReplay to improve chance of optimal path being followed\n",
        "                    if actionhistory is not None:\n",
        "                        MemoryReplay(env = copy.deepcopy(env), bestactionhistory = actionhistory, **kwargs)\n",
        "    name = kwargs['agent'].__name__\n",
        "    if name == 'RandomAgent': \n",
        "        name = 'Random'\n",
        "        bestmemory = '-'\n",
        "    if name == 'QAgent': name = 'Q-Learning'\n",
        "    if name == 'TDAgent': name = 'TD-Learning'\n",
        "    if name == 'GoExplore': name = 'Go-Explore'\n",
        "    if name == 'GoExploreCount': name = 'Go-Explore-Count'\n",
        "    if name == 'CountAgent': name = 'Explore-Count'\n",
        "\n",
        "    if kwargs['agent'] == QAgent or kwargs['agent'] == TDAgent:\n",
        "        if kwargs.get('eps', 1) == 0:\n",
        "            name += ' (Test)'\n",
        "        else:\n",
        "            name += ' (Train)'\n",
        "            \n",
        "    if kwargs.get('intrinsic_fn', None) is not None:\n",
        "        name += ' GDIR'\n",
        "    # if solvedcount == 0:\n",
        "    #     print(f\"{name} & {solvedcount}/{tries} & - & - & - & - & - \\\\\\\\\")\n",
        "    # else:\n",
        "    #     print(f\"{name} & {solvedcount}/{tries} & {firstsolve} & {bestmemory} & {sum(stephistory)/len(stephistory):.1f} & {min(stephistory):.1f} & {max(stephistory):.1f} \\\\\\\\\")\n",
        "    if solvedcount == 0:\n",
        "        print(f'Agent: {name}, No solves at all, First Solve Memory: {bestmemory}, Total Memory: {len(memory)}')\n",
        "    else:\n",
        "        print(f'Agent: {name}, Solve rate: {solvedcount}/{tries} ({solvedcount/tries*100:.1f}%), First Solve: {firstsolve}, First Solve Memory: {bestmemory}, Steps: Avg {sum(stephistory)/len(stephistory):.1f}, Min {min(stephistory):.1f}, Max {max(stephistory):.1f}')"
      ],
      "id": "1f7ffe41-7222-4308-8070-1c4f5a0e6b5f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daca7925-e680-4bab-94d0-7392c8c79abe"
      },
      "source": [
        "# Discrete Environments 1 & 2 - Maze Environment (Unwalled, Walled)"
      ],
      "id": "daca7925-e680-4bab-94d0-7392c8c79abe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bfc749b-97af-48a2-a6ff-c4bdd1bd2eb7"
      },
      "outputs": [],
      "source": [
        "class MazeEnv:\n",
        "    def __init__(self, height=20, width=20, numbricks = 10, grid = None, doorpos = None, agentpos = None, randomseed = None):\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.doorpos = doorpos\n",
        "        self.agentpos = agentpos\n",
        "        self.numbricks = numbricks\n",
        "        self.randomseed = randomseed\n",
        "        self.numsteps = 0\n",
        "        self.done = False\n",
        "        self.reward = 0\n",
        "        if self.randomseed is not None:\n",
        "            np.random.seed(self.randomseed)\n",
        "        self.mapping = {0: '.', 1: 'X', 2: 'D', 3: '#'}\n",
        "        \n",
        "        # if grid not defined, do a random initialization of maze\n",
        "        if grid is None:\n",
        "            self.grid = np.zeros((self.height, self.width))\n",
        "            \n",
        "            # Step 1: get a door position that is valid\n",
        "            if doorpos is None:\n",
        "                self.doorpos = self.getvalidpos()\n",
        "            else:\n",
        "                self.doorpos = doorpos\n",
        "            self.grid[self.doorpos] = 2\n",
        "\n",
        "            # Step 2: get a start position that is valid\n",
        "            if agentpos is None:\n",
        "                self.agentpos = self.getvalidpos()\n",
        "            else:\n",
        "                self.agentpos = agentpos\n",
        "            self.grid[self.agentpos] = 1\n",
        "            \n",
        "            # Step 3: fill in the bricks\n",
        "            for i in range(self.numbricks):\n",
        "                self.grid[self.getvalidpos()] = 3\n",
        "                \n",
        "        # if grid predefined, get the parameters from there instead\n",
        "        else:\n",
        "            self.grid = grid\n",
        "            self.height, self.width = self.grid.shape\n",
        "            \n",
        "            lista, listb = np.where(self.grid == 2)\n",
        "            if len(lista) == 0 or len(listb) == 0:\n",
        "                self.doorpos = self.getvalidpos()\n",
        "            else:\n",
        "                self.doorpos = (lista[0], listb[0])\n",
        "            self.grid[self.doorpos] = 2\n",
        "            \n",
        "            lista, listb = np.where(self.grid == 1)\n",
        "            if len(lista) == 0 or len(listb) == 0:\n",
        "                self.agentpos = self.getvalidpos()\n",
        "            else:\n",
        "                self.agentpos = (lista[0], listb[0])\n",
        "            self.grid[self.agentpos] = 1\n",
        "            \n",
        "        # some variables to reset the environment\n",
        "        self.startgrid = self.grid.copy()\n",
        "        self.startagentpos = self.agentpos\n",
        "        self.startdoorpos = self.doorpos\n",
        "            \n",
        "    def reset(self):\n",
        "        self.grid = self.startgrid.copy()\n",
        "        self.agentpos = self.startagentpos\n",
        "        self.doorpos = self.startdoorpos\n",
        "        self.done = False\n",
        "        self.reward = 0\n",
        "        self.numsteps = 0\n",
        "        if self.randomseed is not None:\n",
        "            np.random.seed(self.randomseed)\n",
        "            \n",
        "    # gets state representation\n",
        "    def staterep(self):\n",
        "        return str(self.agentpos)\n",
        "            \n",
        "    # gets a valid position\n",
        "    def getvalidpos(self):\n",
        "        validpos = []\n",
        "        for i in range(self.height):\n",
        "            for j in range(self.width):\n",
        "                if self.grid[i,j] == 0:\n",
        "                    validpos.append((i,j))\n",
        "        return validpos[np.random.randint(len(validpos))]\n",
        "        \n",
        "    # checks if a position is valid that is not out of the grid and not occupied\n",
        "    def isvalid(self, pos, allowdoor = False):\n",
        "        if pos == None or len(pos)!=2:\n",
        "            return False\n",
        "        height, width = pos\n",
        "        if height < 0 or height >= self.height or width < 0 or width >= self.width:\n",
        "            return False\n",
        "        if allowdoor and self.grid[height,width] == 2:\n",
        "            return True\n",
        "        if self.grid[height,width] == 0:\n",
        "            return True\n",
        "        return False\n",
        "    \n",
        "    def step(self, move):\n",
        "        validmoves = self.getvalidmoves()\n",
        "        # randomly sample a move if not in validmoves\n",
        "        if move not in validmoves:\n",
        "            move = validmoves[np.random.randint(len(validmoves))]\n",
        "        self.numsteps += 1\n",
        "        self.grid[self.agentpos] = 0\n",
        "        self.agentpos = self.movedir(self.agentpos, move)\n",
        "        if self.agentpos == self.doorpos:\n",
        "            self.done = True\n",
        "            self.reward = 1\n",
        "        self.grid[self.agentpos] = 1\n",
        "    \n",
        "    def movedir(self, pos, d):\n",
        "        if pos == None or len(pos)!=2:\n",
        "            return False\n",
        "        height, width = pos\n",
        "        if d=='left':\n",
        "            return (height, width-1)\n",
        "        elif d=='right':\n",
        "            return (height, width+1)\n",
        "        elif d=='up':\n",
        "            return (height-1, width)\n",
        "        elif d=='down':\n",
        "            return (height+1, width)\n",
        "    \n",
        "    def getvalidmoves(self):\n",
        "        validmoves = []\n",
        "        for move in ['left', 'right', 'up', 'down']:\n",
        "            if self.isvalid(self.movedir(self.agentpos, move), allowdoor = True):\n",
        "                validmoves.append(move)\n",
        "        return validmoves\n",
        "    \n",
        "    def sample(self):\n",
        "        return np.random.choice(self.getvalidmoves())\n",
        "    \n",
        "    def print(self):\n",
        "        for i in range(self.height):\n",
        "            for j in range(self.width):\n",
        "                print(self.mapping[self.grid[i,j]], end = '')\n",
        "            print()"
      ],
      "id": "3bfc749b-97af-48a2-a6ff-c4bdd1bd2eb7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4907ded4-bf6b-4a23-8b2e-95dddf9f4e7b"
      },
      "outputs": [],
      "source": [
        "def Manhattan(env):\n",
        "    ''' Calculates the Manhattan distance between the agent and the door '''\n",
        "    pointA = env.agentpos\n",
        "    pointB = env.doorpos\n",
        "    return -(abs(pointA[0]-pointB[0])+abs(pointA[1]-pointB[1]))/(env.width+env.height-2)"
      ],
      "id": "4907ded4-bf6b-4a23-8b2e-95dddf9f4e7b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94c2cd90-ca17-4b85-8b1d-4993e73bfd83"
      },
      "source": [
        "## Unwalled maze (10x10)"
      ],
      "id": "94c2cd90-ca17-4b85-8b1d-4993e73bfd83"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55b99f46-6c75-401a-a4c4-4a05f15c7abe"
      },
      "outputs": [],
      "source": [
        "# This is how the maze looks like\n",
        "height, width = 10, 10\n",
        "env = MazeEnv(height = height, width = width, agentpos = (0, 0), doorpos = (height-1, width-1), randomseed = 1, numbricks = height*width//10)\n",
        "env.print()"
      ],
      "id": "55b99f46-6c75-401a-a4c4-4a05f15c7abe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c79753b3-35ee-4131-9a66-74a570eb1bc0"
      },
      "outputs": [],
      "source": [
        "for agent in [RandomAgent, TDAgent, QAgent]:\n",
        "    # print(agent.__name__, 'training')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False)\n",
        "    \n",
        "    # for QAgent and TDAgent, we include the final number of solves with deterministic transition\n",
        "    if agent == QAgent or agent == TDAgent:\n",
        "        # print(agent.__name__, 'testing')\n",
        "        MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, eps = 0)\n",
        "\n",
        "for agent in [GoExplore, GoExploreCount, CountAgent]:\n",
        "    # print(agent.__name__, 'without IM')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, intrinsic_fn = None)\n",
        "\n",
        "    # for Intrinsic Agent, we also do the run with intrinsic motivation\n",
        "    # print(agent.__name__, 'with IM')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, intrinsic_fn = Manhattan)"
      ],
      "id": "c79753b3-35ee-4131-9a66-74a570eb1bc0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3906a13-114b-4dfa-9330-298a1ecf5430"
      },
      "source": [
        "## Unwalled maze (20x20)"
      ],
      "id": "c3906a13-114b-4dfa-9330-298a1ecf5430"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fa0f838-6196-4dd3-a830-26ff49bbcc2e"
      },
      "outputs": [],
      "source": [
        "# This is how the maze looks like\n",
        "height, width = 20, 20\n",
        "env = MazeEnv(height = height, width = width, agentpos = (0, 0), doorpos = (height-1, width-1), randomseed = 1, numbricks = height*width//10)\n",
        "env.print()"
      ],
      "id": "5fa0f838-6196-4dd3-a830-26ff49bbcc2e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7129fb89-8201-4c0e-8e31-9ba26e7f3135"
      },
      "outputs": [],
      "source": [
        "for agent in [RandomAgent, TDAgent, QAgent]:\n",
        "    # print(agent.__name__, 'training')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False)\n",
        "    \n",
        "    # for QAgent and TDAgent, we include the final number of solves with deterministic transition\n",
        "    if agent == QAgent or agent == TDAgent:\n",
        "        # print(agent.__name__, 'testing')\n",
        "        MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, eps = 0)\n",
        "    \n",
        "for agent in [GoExplore, GoExploreCount, CountAgent]:\n",
        "    # print(agent.__name__, 'without IM')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, intrinsic_fn = None)\n",
        "\n",
        "    # for Intrinsic Agent, we also do the run with intrinsic motivation\n",
        "    # print(agent.__name__, 'with IM')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, intrinsic_fn = Manhattan)"
      ],
      "id": "7129fb89-8201-4c0e-8e31-9ba26e7f3135"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "422c28c5-f847-473d-98e2-f8d32902e78f"
      },
      "source": [
        "## Unwalled maze (100x100)"
      ],
      "id": "422c28c5-f847-473d-98e2-f8d32902e78f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c7d3007-7931-4ac0-9de6-9064d9c265ac"
      },
      "outputs": [],
      "source": [
        "# This is how the maze looks like\n",
        "height, width = 100, 100\n",
        "env = MazeEnv(height = height, width = width, agentpos = (0, 0), doorpos = (height-1, width-1), randomseed = 1, numbricks = height*width//10)\n",
        "env.print()"
      ],
      "id": "4c7d3007-7931-4ac0-9de6-9064d9c265ac"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67f416d9-bec1-444d-863a-b893ef4b96ae"
      },
      "outputs": [],
      "source": [
        "for agent in [RandomAgent, TDAgent, QAgent]:\n",
        "    # print(agent.__name__, 'training')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False)\n",
        "    \n",
        "    # for QAgent and TDAgent, we include the final number of solves with deterministic transition\n",
        "    if agent == QAgent or agent == TDAgent:\n",
        "        # print(agent.__name__, 'testing')\n",
        "        MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, eps = 0)\n",
        "    \n",
        "for agent in [GoExplore, GoExploreCount, CountAgent]:\n",
        "    # print(agent.__name__, 'without IM')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, intrinsic_fn = None)\n",
        "\n",
        "    # for Intrinsic Agent, we also do the run with intrinsic motivation\n",
        "    # print(agent.__name__, 'with IM')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, intrinsic_fn = Manhattan)"
      ],
      "id": "67f416d9-bec1-444d-863a-b893ef4b96ae"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7c81719-3778-495c-8f5c-31adc521b217"
      },
      "source": [
        "## Extra: Without hippocampal replay"
      ],
      "id": "a7c81719-3778-495c-8f5c-31adc521b217"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1046f337-03da-4abd-b157-97862221a2f0"
      },
      "outputs": [],
      "source": [
        "for agent in [RandomAgent, TDAgent, QAgent]:\n",
        "    # print(agent.__name__, 'training')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, hippocampal_replay = False, verbose = False)\n",
        "    \n",
        "    # for QAgent and TDAgent, we include the final number of solves with deterministic transition\n",
        "    if agent == QAgent or agent == TDAgent:\n",
        "        # print(agent.__name__, 'testing')\n",
        "        MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, hippocampal_replay = False, verbose = False, eps = 0)\n",
        "    \n",
        "for agent in [GoExplore, GoExploreCount, CountAgent]:\n",
        "    # print(agent.__name__, 'without IM')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, hippocampal_replay = False, verbose = False, intrinsic_fn = None)\n",
        "\n",
        "    # for Intrinsic Agent, we also do the run with intrinsic motivation\n",
        "    # print(agent.__name__, 'with IM')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, hippocampal_replay = False, verbose = False, intrinsic_fn = Manhattan)"
      ],
      "id": "1046f337-03da-4abd-b157-97862221a2f0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3b91b2c-2960-4b18-8fcd-808902e97a50"
      },
      "source": [
        "## Walled maze (10x10)"
      ],
      "id": "b3b91b2c-2960-4b18-8fcd-808902e97a50"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "644c7e52-d8f5-4309-a450-965af5e2f971"
      },
      "outputs": [],
      "source": [
        "# create game environment\n",
        "size = 10\n",
        "grid = np.zeros((size,size))\n",
        "maxheight, maxwidth = grid.shape\n",
        "grid[:, maxwidth//2-1] = 3\n",
        "grid[maxheight//2,:] = 3\n",
        "grid[1:maxheight, maxwidth-2] = 3\n",
        "grid[maxheight//2, maxwidth//4] = 0\n",
        "grid[maxheight//2, 3*maxwidth//4-1] = 0\n",
        "grid[3*maxheight//4, maxwidth//2-1] = 0\n",
        "grid[maxheight//2, maxwidth-1] = 0\n",
        "grid[0, 0] = 1\n",
        "grid[maxheight-1, maxwidth-1] = 2\n",
        "env = MazeEnv(grid = grid)\n",
        "env.print()"
      ],
      "id": "644c7e52-d8f5-4309-a450-965af5e2f971"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91f53743-83f1-46b5-bbea-b950e2738e52"
      },
      "outputs": [],
      "source": [
        "for agent in [RandomAgent, TDAgent, QAgent]:\n",
        "    # print(agent.__name__, 'training')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False)\n",
        "    \n",
        "    # for QAgent and TDAgent, we include the final number of solves with deterministic transition\n",
        "    if agent == QAgent or agent == TDAgent:\n",
        "        # print(agent.__name__, 'testing')\n",
        "        MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, eps = 0)\n",
        "    \n",
        "for agent in [GoExplore, GoExploreCount, CountAgent]:\n",
        "    # print(agent.__name__, 'without IM')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, intrinsic_fn = None)\n",
        "\n",
        "    # for Intrinsic Agent, we also do the run with intrinsic motivation\n",
        "    # print(agent.__name__, 'with IM')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, intrinsic_fn = Manhattan)"
      ],
      "id": "91f53743-83f1-46b5-bbea-b950e2738e52"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "103911ef-dd19-4146-8457-9116c2c352e6"
      },
      "source": [
        "## Walled maze (20x20)"
      ],
      "id": "103911ef-dd19-4146-8457-9116c2c352e6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b2f7559-5cc6-4556-8a8b-543cbe8e6cd4"
      },
      "outputs": [],
      "source": [
        "# create game environment\n",
        "size = 20\n",
        "grid = np.zeros((size,size))\n",
        "maxheight, maxwidth = grid.shape\n",
        "grid[:, maxwidth//2-1] = 3\n",
        "grid[maxheight//2,:] = 3\n",
        "grid[1:maxheight, maxwidth-2] = 3\n",
        "grid[maxheight//2, maxwidth//4] = 0\n",
        "grid[maxheight//2, 3*maxwidth//4-1] = 0\n",
        "grid[3*maxheight//4, maxwidth//2-1] = 0\n",
        "grid[maxheight//2, maxwidth-1] = 0\n",
        "grid[0, 0] = 1\n",
        "grid[maxheight-1, maxwidth-1] = 2\n",
        "env = MazeEnv(grid = grid)\n",
        "env.print()"
      ],
      "id": "3b2f7559-5cc6-4556-8a8b-543cbe8e6cd4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d3fdd4b-b771-4934-9d74-88c8639a6763"
      },
      "outputs": [],
      "source": [
        "for agent in [RandomAgent, TDAgent, QAgent]:\n",
        "    # print(agent.__name__, 'training')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False)\n",
        "    \n",
        "    # for QAgent and TDAgent, we include the final number of solves with deterministic transition\n",
        "    if agent == QAgent or agent == TDAgent:\n",
        "        # print(agent.__name__, 'testing')\n",
        "        MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, eps = 0)\n",
        "    \n",
        "for agent in [GoExplore, GoExploreCount, CountAgent]:\n",
        "    # print(agent.__name__, 'without IM')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, intrinsic_fn = None)\n",
        "\n",
        "    # for Intrinsic Agent, we also do the run with intrinsic motivation\n",
        "    # print(agent.__name__, 'with IM')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, intrinsic_fn = Manhattan)"
      ],
      "id": "6d3fdd4b-b771-4934-9d74-88c8639a6763"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "560e37e2-aaea-416b-8d9f-652da51aa602"
      },
      "source": [
        "## Walled maze (100x100)"
      ],
      "id": "560e37e2-aaea-416b-8d9f-652da51aa602"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9db7318b-2126-4ea0-96b6-9ef2715adc49"
      },
      "outputs": [],
      "source": [
        "# create game environment\n",
        "size = 100\n",
        "grid = np.zeros((size,size))\n",
        "maxheight, maxwidth = grid.shape\n",
        "grid[:, maxwidth//2-1] = 3\n",
        "grid[maxheight//2,:] = 3\n",
        "grid[1:maxheight, maxwidth-2] = 3\n",
        "grid[maxheight//2, maxwidth//4] = 0\n",
        "grid[maxheight//2, 3*maxwidth//4-1] = 0\n",
        "grid[3*maxheight//4, maxwidth//2-1] = 0\n",
        "grid[maxheight//2, maxwidth-1] = 0\n",
        "grid[0, 0] = 1\n",
        "grid[maxheight-1, maxwidth-1] = 2\n",
        "env = MazeEnv(grid = grid)\n",
        "env.print()"
      ],
      "id": "9db7318b-2126-4ea0-96b6-9ef2715adc49"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97c2936b-e682-4479-bbff-8657b3f2725d"
      },
      "outputs": [],
      "source": [
        "for agent in [RandomAgent, TDAgent, QAgent]:\n",
        "    # print(agent.__name__, 'training')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False)\n",
        "    \n",
        "    # for QAgent and TDAgent, we include the final number of solves with deterministic transition\n",
        "    if agent == QAgent or agent == TDAgent:\n",
        "        # print(agent.__name__, 'testing')\n",
        "        MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, eps = 0)\n",
        "    \n",
        "for agent in [GoExplore, GoExploreCount, CountAgent]:\n",
        "    # print(agent.__name__, 'without IM')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, intrinsic_fn = None)\n",
        "\n",
        "    # for Intrinsic Agent, we also do the run with intrinsic motivation\n",
        "    # print(agent.__name__, 'with IM')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, verbose = False, intrinsic_fn = Manhattan)"
      ],
      "id": "97c2936b-e682-4479-bbff-8657b3f2725d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8a45987-56db-40f1-bfb4-5f2e1aaa2005"
      },
      "source": [
        "## Extra: Without hippocampal replay"
      ],
      "id": "f8a45987-56db-40f1-bfb4-5f2e1aaa2005"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee761593-20e0-42ee-887c-f655e5f8a139"
      },
      "outputs": [],
      "source": [
        "for agent in [RandomAgent, TDAgent, QAgent]:\n",
        "    # print(agent.__name__, 'training')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, hippocampal_replay = False, verbose = False)\n",
        "    \n",
        "    # for QAgent and TDAgent, we include the final number of solves with deterministic transition\n",
        "    if agent == QAgent or agent == TDAgent:\n",
        "        # print(agent.__name__, 'testing')\n",
        "        MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, hippocampal_replay = False, verbose = False, eps = 0)\n",
        "    \n",
        "for agent in [GoExplore, GoExploreCount, CountAgent]:\n",
        "    # print(agent.__name__, 'without IM')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, hippocampal_replay = False, verbose = False, intrinsic_fn = None)\n",
        "\n",
        "    # for Intrinsic Agent, we also do the run with intrinsic motivation\n",
        "    # print(agent.__name__, 'with IM')\n",
        "    memory = defaultdict(lambda: 0)\n",
        "    MultiGame(numtries = 100, env = copy.deepcopy(env), agent = agent, maxsteps = env.height*env.width, hippocampal_replay = False, verbose = False, intrinsic_fn = Manhattan)"
      ],
      "id": "ee761593-20e0-42ee-887c-f655e5f8a139"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_6olmsXnAs3V"
      },
      "id": "_6olmsXnAs3V",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "b3b91b2c-2960-4b18-8fcd-808902e97a50",
        "103911ef-dd19-4146-8457-9116c2c352e6"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}